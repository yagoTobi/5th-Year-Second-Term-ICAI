{"cells":[{"cell_type":"markdown","source":["# **INTELIGENCIA DEL NEGOCIO / Business Intelligence**\n","## **ASSIGNMENT A4 - WORD EMBEDDING**\n","\n","**INSTRUCCIONES / RECOMENDACIONES**\n","\n","- Se recomienda leer con detalle la descripción de cada una de las celdas.\n","- Las celdas que ya tienen código, se deberán ejecutar directamente. Nota: existen alguna celdas con código que deberán completarse o parametrizarse.\n","- Las celdas que están vacías, se completarán con la implementación requerida en el notebook.\n","- No se incluirán más celdas de las establecidas en el presente notebook, por lo que la solución al mismo deberá implementarse exclusivamente en las celdas vacías.\n","- Scikit-Learn es un paquete muy útil para las operaciones de preprocesamiento de los datos, como estandarización, normalización, codificación, etc.\n","- La entrega se realizará vía Moodle. Será necesario subir la solución a este notebook con el nombre: **NOMBRE_GRUPO.ipynb**\n","\n","- **Fecha de Publicación: 08/04/2024**\n","- **Fecha de Entrega: 14/04/2024**\n","- **Test: 15/04/2024**\n"],"metadata":{"id":"vQSZ3w0zroV9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbMTlXNjS8Ts"},"outputs":[],"source":["! pip install --upgrade gensim\n","! pip install wordcloud\n","! pip install tqdm"]},{"cell_type":"markdown","metadata":{"id":"gjnrFHmM6KiU"},"source":["Vamos a trabajar con un dataset de titulares de noticias de la ABC en Australia durante 2020 y 2021. Vamos a realizar un análisis de text mining para tratar de saber de que temas trataron estos titulares."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l74hB6KqGlZx"},"outputs":[],"source":["import pandas as pd\n","from wordcloud import WordCloud\n","from functools import reduce\n","from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from gensim.models import Word2Vec\n","import random\n","from tqdm import tqdm"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"gIKatU5phlV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wo_uSgMNGsSd"},"outputs":[],"source":["path = \"abcnews_2020.csv\"\n","pd_data = pd.read_csv(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwxps1QFl9cX"},"outputs":[],"source":["texts = pd_data[\"headline_text\"].values"]},{"cell_type":"markdown","metadata":{"id":"q_WMNK83HTy8"},"source":["# Nube de palabras\n","\n","Vamos a construir una nube de palabras que nos permita visualizar cuáles son las palabras más mencionadas.\n","\n","Para ello, primero debemos construir un diccionario cuya clave sea la palabra y el valor sea el número de palabras que aparece.\n","\n","Vamos a utilizar el <a href=https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html> CountVectorizer </a> de sklearn para ello."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vnGe_JVoIcvv"},"outputs":[],"source":["cv = CountVectorizer(stop_words=\"english\")"]},{"cell_type":"markdown","metadata":{"id":"j2bsmbiWsp6T"},"source":["Ajuste el cv en los textos y consiga la matriz de conteos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlEu8xcmspbB"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_qLDri6dsx-3"},"source":["Ahora, construya el diccionario. Para conseguir la palabra que representa cada columna, use el atributo vocabulary_ de count vectorizer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSr8DeVLIovy"},"outputs":[],"source":["all_words = cv.vocabulary_\n","len(all_words)"]},{"cell_type":"markdown","source":["Obtener el número de frecuencia de las palabras (freqs) para posteriormente utilizarlo a la hora de dibujar la nube."],"metadata":{"id":"3Kllv8t91fgW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDxNlqEBJPlf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"j6wqriHRtINy"},"source":["Finalmente dibujamos la nube:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7m4OGCcdG-H_"},"outputs":[],"source":["wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate_from_frequencies(freqs)\n","plt.figure(figsize=(15, 8))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis('off')\n","plt.title(\"Some frequent words used in the headlines\", weight='bold', fontsize=14)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5eJ92E4RtO5i"},"source":["¿Sabrías descifrar algunos temas frecuentes en los titulares?"]},{"cell_type":"markdown","metadata":{"id":"ZrKLklYAnwQs"},"source":["# Topic Modelling\n","\n","Vamos a vectorizar los textos como la media de las palabras. Para ello, primero vamos a entrenar un word embedding sobre los textos."]},{"cell_type":"markdown","metadata":{"id":"DJ7Dn4pauiid"},"source":["## Word Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmhF4iSsWLAp"},"outputs":[],"source":["from gensim.utils import tokenize\n","def tokenizer(text):\n","  \"\"\"Returns a list of tokens\"\"\"\n","  return tokenize(text, lowercase=True)\n","\n","list(tokenizer(texts[0]))"]},{"cell_type":"markdown","source":["Especificar los parámetros del modelado."],"metadata":{"id":"I_vz3eQ92CuP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJ7Dbb8-M-i7"},"outputs":[],"source":["SIZE_VECTORS =\n","WINDOW =\n","EPOCHS =\n","MIN_COUNT ="]},{"cell_type":"markdown","source":["Entrenar el modelo de embeddings..."],"metadata":{"id":"s6IAcWjd2OVq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoCDYHp5NqrJ"},"outputs":[],"source":["we_model = Word2Vec(...)"]},{"cell_type":"markdown","metadata":{"id":"tv64HLBouTss"},"source":["Elija algunas palabras de la nube y muestre sus palabras más cercanas. ¿Aparecen resultados con sentido?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tliPZvSMuewy"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"a3FNaWOEusoh"},"source":["# Sentence Embedding\n","\n","Calculamos el vector de un texto como la media de los vectores de sus palabras."]},{"cell_type":"markdown","metadata":{"id":"xMuiCIubvGsn"},"source":["Creamos la función que genera el vector.\n","Hay que tener cuidado con algunos casos especiales:\n","- Puede que no conozcamos el vector para alguna de las palabras. En ese caso la ignoramos y calculamos la media del resto de vectores.\n","- En el caso de que no conozcamos el vector de ninguna de las palabras, devolveremos None."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4UmL3XcNDii"},"outputs":[],"source":["def vectorize_text(text):\n","  \"\"\"Converts a text into a vector\"\"\"\n","  tokenized = tokenizer(text)\n","\n","  vectors = []\n","  for w in tokenized:\n","    try:\n","        v = we_model.wv[w]\n","        vectors.append(v)\n","    except KeyError:\n","        pass\n","    return (text, np.mean(vectors, axis=0)) if len(vectors) else None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gw_OmIMUktOO"},"outputs":[],"source":["texts_with_vectors = [vectorize_text(t) for t in tqdm(texts)]\n","texts_with_vectors = [x for x in texts_with_vectors if x is not None] # Removes samples on which we could not compute the vector of the sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m487lBYJRCRW"},"outputs":[],"source":["texts_topic_modelling, vectors = zip(*texts_with_vectors)"]},{"cell_type":"markdown","metadata":{"id":"hdp9-Us8vq4I"},"source":["## Clustering\n","\n","Realice clustering usando KMeans sobre los vectores generados.\n","Elija el número de clusters usando el método del codo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgz_xZOcpdYw"},"outputs":[],"source":["from sklearn.cluster import KMeans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWoWigMJORBY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfq9ZtiQPO4H"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"DHfgVhIDv0UH"},"source":["## Nube de palabras para cada cluster\n","\n","Para cada uno de los clusters generados:\n","- Tome todos los textos en ese cluster\n","- Dibuje la nube de palabras de dichos textos.\n","\n","Debería observar que cada cluster habla de una temática distinta. ¿Puede explicar el significado de alguno de ellos?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctRSrxvbplKI"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"oK4KMOwkVq0L"},"source":["# Reducción de vocabulario\n","\n","Algunas nubes puede que hayan salido demasiado confusas porque palabras con semántica similar aparecen repetidas.\n","\n","Vamos a detectar cuales tienen la misma semántica y agruparlas en una sola."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BKuV4F2IVqr4"},"outputs":[],"source":["from scipy.cluster.hierarchy import linkage,  dendrogram, cut_tree\n","from gensim.parsing.preprocessing import STOPWORDS\n","\n","all_stop_words = list(STOPWORDS) + [\"afl\"] # appears in a lot of texts\n","we_model.wv.sort_by_descending_frequency() # Orders vocab in the model by descending order\n","\n","all_words = np.asarray([w for w in list(we_model.wv.key_to_index.keys()) if w not in all_stop_words])\n"]},{"cell_type":"markdown","metadata":{"id":"hvuTg37oxNhG"},"source":["Vamos a ejecutar clustering jerárquico sobre las palabras para agrupar los sinónimos. Para elegir el threshold, tomamos unas pocas palabras y ejecutamos el algoritmo de clustering en ellas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-HMsOT4LWVmx"},"outputs":[],"source":["some_words = [\"covid\", \"coronavirus\", \"president\", \"trump\", \"biden\", \"pandemic\", \"market\", \"markets\", \"quarantine\", \"restrictions\", \"lockdown\", \"news\", \"updates\"]\n","vectors_of_words = [we_model.wv[w] for w in some_words]\n","Z = linkage(vectors_of_words,  metric=\"cosine\", method=\"complete\" )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeI-8vabWwHJ"},"outputs":[],"source":["plt.figure(figsize=(20, 10))\n","dn = dendrogram(Z, labels=some_words)\n","plt.grid(True)"]},{"cell_type":"markdown","metadata":{"id":"TmVCwJzVxcr3"},"source":["Elija un valor de threshold adecuado que pueda agrupar palabras con la misma semántica."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nu9AQv2BxigU"},"outputs":[],"source":["THRESHOLD ="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Srlwe5VxZoZy"},"outputs":[],"source":["vectors_of_words = [we_model.wv[w] for w in all_words]\n","Z = linkage(vectors_of_words,  metric=\"cosine\", method=\"complete\")\n","cluster_words = cut_tree(Z, height=THRESHOLD).flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3Bm2YD7Z_yQ"},"outputs":[],"source":["obtained_clusters = max(cluster_words)\n","obtained_clusters"]},{"cell_type":"markdown","metadata":{"id":"9xdIZwONxpdJ"},"source":["Construya un diccionario que tenga por clave el número de cluster y por valor la lista de todas las palabras en dicho cluster."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dp8cT83Vaabt"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-Zh43N6xx0s7"},"source":["Construya un segundo diccionario con las traducciones que se deben realizar.\n","\n","Es decir, si la palabra \"covid\" debe convertirse en \"coronavirus\", entonces el diccionario tendra una entrada como:\n","\n","{\n","  \"covid\": \"coronavirus\"\n","}\n","\n","Como representante de cada cluster, tome la palabra de mayor conteo (recuerde que en all_words están ordenadas por conteo)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qY6hj1jdUTW"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"6izTMkQMzW7w"},"source":["Compruebe algunas entradas del diccionario para comprobar que salen cosas con sentido."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4XZ_W1i0XSWT"},"outputs":[],"source":["def replace_words_in_text(text):\n","  tokens = tokenizer(text)\n","  tokens_replacement = [dict_replacements.get(t, t) for t in tokens]\n","  return \" \".join(tokens_replacement)"]},{"cell_type":"markdown","metadata":{"id":"K0NOvr38zHM_"},"source":["Aplique la reducción de vocabulario a cada uno de los textos. Vuelva a dibujar las nubes de cada cluster pero sobre las frases con vocabulario reducido. Comente resultados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aivkdjSgd-U6"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Conclusiones del estudio\n","\n","Detallar las principales conclusiones extraidas sobre la aplicación de técnicas de word embedding para el tratamiento de textos."],"metadata":{"id":"OnmTZWbGsn88"}},{"cell_type":"markdown","source":["*Escribir AQUI las conclusiones*"],"metadata":{"id":"YSuabUu2s36y"}}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"deeplearning","language":"python","name":"deeplearning"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}