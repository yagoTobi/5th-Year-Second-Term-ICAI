{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chuletario Machine Learning III - Yago Tobio Souto\n",
    "---\n",
    "## Indice: \n",
    "\n",
    "+ [**EDA**](#section1)\n",
    "  + [Import & File Previews](#eda-section1)\n",
    "  + [Agrupación, data types, datos unicos](#eda-section2)\n",
    "  + [Outliers](#eda-section3)\n",
    "  + [Sparsity](#eda-section4)\n",
    "  + [Recommendation Systems EDA](#eda-section5)\n",
    "+ [**PCA**](#section2)\n",
    "  + [Preparación del dataset y normalizar](#pca-section1)\n",
    "  + [Generar latent dims, aplicar PCA, determinar error](#pca-section2)\n",
    "  + [Explicación del modelo - autovalores, varianza](#pca-section3)\n",
    "  + [Averiguar latent dim optimo haciendo plots](#pca-section4)\n",
    "+ [**PCA combinado con Clustering**](#important-exercise)\n",
    "+ [**Clustering**](#section3)\n",
    "  + [Data preparation](#clustering-section1)\n",
    "  + [Calculations (Silhouette-score, Rand Index, Mutual Info, etc...)](#clustering-section2)\n",
    "  + [HAC](#clustering-section3)\n",
    "  + [K-Means Clustering](#clustering-section4)\n",
    "  + [Mini-batches](#clustering-section5)\n",
    "  + [Mixture Models](#clustering-section6)\n",
    "\n",
    "+ [**Memory-Based Filtering**](#section4)\n",
    "  + [EDA](#rs-section0)\n",
    "  + [User-Based Filtering](#rs-section1)\n",
    "  + [Item-Based Filtering](#rs-section2)\n",
    "  \n",
    "+ [**Model-Based Filtering**](#section5)\n",
    "  + [EDA](@rs-section00)\n",
    "  + [Singular Value Decomposition](#rs-section3)\n",
    "  + [Matrix Factorisation](#rs-section4)\n",
    "\n",
    "+ [**Implicit Feedback**](#section6)\n",
    "  + [BPR - Bayesian Personalised Ranking](#rs-section5)\n",
    "  + [WMF - Weighted Matrix Factorisation](#rs-section6)\n",
    "  + [FM - Factorisation Machines](#rs-section7)\n",
    "  \n",
    "+ [**Natural Language Processing**](#section7)\n",
    "  + [Intro](#nlp-section0)\n",
    "  + [VADER](#nlp-section1)\n",
    "  + [ROBERTA](#nlp-section2)\n",
    "  \n",
    "+ [**Network Analysis**](#section8)\n",
    "  + [Graph Modality - SoRec](#na-section1)\n",
    "  + [Text Modality - CTR (Collaborative Topic Regression)](#na-section2)\n",
    "\n",
    "[*Para que depois muchos digan que no se trabaya...*](https://www.youtube.com/watch?v=Zcb8yPEItwA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Librerías**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:47) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "Cornac version: 2.1\n",
      "TensorFlow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data manipulation and numerical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse as sp, stats\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Text and natural language processing\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Machine Learning and Data Science libraries\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import mean_squared_error, silhouette_samples, silhouette_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, normalized_mutual_info_score, rand_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "# Graph and plot libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Transformer models\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Utility and other specific tools\n",
    "from adjustText import adjust_text\n",
    "from tabulate import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Recommender systems and specific utilities\n",
    "import cornac\n",
    "from cornac.data.text import BaseTokenizer\n",
    "from cornac.data import GraphModality, ImageModality, TextModality, text as cornac_text\n",
    "from cornac.datasets import amazon_clothing, filmtrust, movielens\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.models import BPR, BaselineOnly, CTR, ItemKNN, MF, NMF, PMF, SVD, SoRec, UserKNN, VBPR, WMF\n",
    "from cornac.utils import cache\n",
    "#from cornac.datasets.python_splitters import python_random_split\n",
    "#from cornac.models.cornac.cornac_utils import predict_ranking\n",
    "\n",
    "# Elasticsearch\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "#from recommenders.utils.timer import Timer\n",
    "#from recommenders.datasets import movielens\n",
    "#from recommenders.utils.notebook_utils import store_metadata\n",
    "#from recommenders.models.recommenders.utils.constants import SEED\n",
    "#from recommenders.datasets.python_splitters import python_random_split\n",
    "#from recommenders.evaluation.python_evaluation import (\n",
    "#    map,\n",
    "#    ndcg_at_k,\n",
    "#    precision_at_k,\n",
    "#    recall_at_k,\n",
    "#)\n",
    "\n",
    "# Constants and configurations\n",
    "SEED = 42\n",
    "VERBOSE = False\n",
    "pd.set_option(\"max_colwidth\", 0)\n",
    "%matplotlib inline\n",
    "\n",
    "# Printing versions of libraries\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Cornac version: {cornac.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Import-file + pre-processing) <a id=\"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import & File Previews <a id='eda-section1'></a>\n",
    "* Import file\n",
    "* Preview file \n",
    "* Eliminar columnas\n",
    "* Sample de los datos\n",
    "* Copiar un dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Import file\n",
    "df_path = \"df_path\"\n",
    "df = pd.read_csv(df_path, header=0)\n",
    "\n",
    "# * - Preview, Resumen de caracteristicas, num filas x columns.\n",
    "df.head()\n",
    "df.info()\n",
    "df.shape\n",
    "\n",
    "# * - Eliminar columnas\n",
    "df.drop(\"Timestamp\", axis=1, inplace=True)\n",
    "\n",
    "# * - Ubicar los na's\n",
    "df.isna().sum()\n",
    "\n",
    "# * - Coger una muestra aleatoria de los datos\n",
    "df_sample = df.sample(n=10000, random_state=42, ignore_index=True)\n",
    "\n",
    "# * - Copiar un dataset\n",
    "df_X = df_sample.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agrupación, data types, datos unicos <a id='eda-section2'></a>\n",
    "\n",
    "* Proporción de Ratings\n",
    "* Unique - Conteo y valores unicos \n",
    "* Agrupación de columnas\n",
    "* Visualizar los data types\n",
    "* Obten los productos con mayor rating count \n",
    "* THRESHOLD DEL INTER - Conservar solo productos con un minimo de criticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO - Añadir grafico de barras\n",
    "# ? - Hallar el número de ratings individuales - (Útil para observar proporción y escala de los ratings)\n",
    "df_sample.value_counts(\"Rating\", normalize=True)\n",
    "\n",
    "column_unique_values = df_sample[\n",
    "    \"column\"\n",
    "].unique()  # ? - Esto obtiene los valores únicos por col\n",
    "\n",
    "number_column_unique_values = df_sample[\n",
    "    \"column\"\n",
    "].nunique()  # ? - Esto el número de valores únicos\n",
    "\n",
    "# * - Agrupación de columnas:\n",
    "group_by_and_count = pd.DataFrame(df_sample.groupby(\"ProductId\")[\"Rating\"].count())\n",
    "sorted_values_by_criteria = group_by_and_count.sort_values(\"Rating\", ascending=False)\n",
    "sorted_values_by_criteria.head(10)\n",
    "#! - Esto de abajo y lo de arriba, hacen lo mismo\n",
    "# * - Obten los productos con mayor número de críticas\n",
    "item_rate_count = (\n",
    "    df_sample.groupby(\"ProductId\")[\"UserId\"].nunique().sort_values(ascending=False)\n",
    ")\n",
    "item_rate_count  # ? - Get the number of reviews for a product\n",
    "\n",
    "# * - Meter info previa en un dataFrame:\n",
    "unique_counts = df_sample.nunique()\n",
    "unique_values = [df_sample[column].unique() for column in df_sample.columns]\n",
    "\n",
    "# * - Obten en array los tipos de datos por columna:\n",
    "data_types = [str(df_sample[column].dtype) for column in df_sample.columns]\n",
    "\n",
    "unique_counts_df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": df_sample.columns,\n",
    "        \"unique_count\": unique_counts,\n",
    "        \"unique_values\": unique_values,\n",
    "        \"data_type\": data_types,\n",
    "    }\n",
    ")\n",
    "unique_counts_df\n",
    "\n",
    "# ! - THRESHOLD DEL INTER:  Filtrar el conjunto de datos, y conservar aquellos productos con al menos 20 criticas:\n",
    "reviews_per_rating = df_sample[[\"productId\", \"rating\"]].value_counts()\n",
    "select_product = (reviews_per_rating >= 20).groupby(\"productId\").all()\n",
    "select_product = select_product.index[select_product].to_list()\n",
    "df = df_sample.loc[df_sample[\"productId\"].isin(select_product)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers <a id='eda-section3'></a>\n",
    "\n",
    "* Función de análisis de outliers de ratings\n",
    "* Obtener los outliers y visualizar el boxplot\n",
    "* Obtener porcentaje de outliers de nuestra muestra\n",
    "* Quitar los outliers en caso de que no haya sesgo evidente en el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_outliers(df, num_vars):\n",
    "    \"\"\"\n",
    "    Explora y identifica los valores atípicos de variables numéricas en un DataFrame.\n",
    "\n",
    "    Retorna:\n",
    "    - outliers_df (diccionario): Diccionario con las variables numéricas como claves. Cada valor es otro diccionario\n",
    "      con las claves 'values' (valores atípicos), 'positions' (posiciones de los valores atípicos en el DataFrame)\n",
    "      e 'indices' (índices de los valores atípicos en el DataFrame).\n",
    "    \"\"\"\n",
    "    outliers_df = dict()\n",
    "    for k in range(len(num_vars)):\n",
    "        var = num_vars[k]\n",
    "        sns.boxplot(df, x=var)\n",
    "        outliers_df[var] = boxplot_stats(df[var])[0][\n",
    "            \"fliers\"\n",
    "        ]  # ? - Boxplot de TODOS LOS RATINGS EN NUESTRA MUESTRA\n",
    "        out_pos = np.where(df[var].isin(outliers_df[var]))[0].tolist()\n",
    "        out_idx = [df[var].index.tolist()[k] for k in out_pos]\n",
    "        outliers_df[var] = {\n",
    "            \"values\": outliers_df[var],\n",
    "            \"positions\": out_pos,\n",
    "            \"indices\": out_idx,\n",
    "        }\n",
    "    return outliers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Obtener los outliers y visualizar el boxplot.\n",
    "outlier_ratings = explore_outliers(df_sample, [\"Rating\"])\n",
    "\n",
    "# * Obtener porcentaje de outliers de nuestra muestra:\n",
    "print(\n",
    "    \"Percentage of outliers:\",\n",
    "    round(len(outlier_ratings.get(\"Rating\").get(\"indices\")) / len(df_sample), 3) * 100,\n",
    "    \"%\",\n",
    ")\n",
    "\n",
    "# ! - Si hay un sesgo muy claro en el boxplot, NO recomendamos quitar las anomalías para\n",
    "# ! - capturar todos los comportamientos posibles de usuarios.\n",
    "\n",
    "# * En caso de querer quitar los outliers:\n",
    "df_sample.drop(outlier_ratings.get(\"Rating\").get(\"indices\"), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsity <a id='eda-section4'></a>\n",
    "\n",
    "* Calculo de Sparsity - Indica longtail property. (Si el sparsity es alto, optamos por cosine similarity)\n",
    "* Plot para observar si existe el Long-Tail Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * - Calculo de Sparsity. Nos dice si nuestro dataset exhibe propiedades long-tail\n",
    "# * - Si el sparsity es alto, yo optaría por hacer Cosine similarity\n",
    "# * - Como de llena esta nuestra matriz de ratings:\n",
    "def print_sparsity(df):\n",
    "    n_users = df.UserId.nunique()\n",
    "    n_items = df.ProductId.nunique()\n",
    "    n_ratings = len(df)\n",
    "    rating_matrix_size = n_users * n_items\n",
    "    sparsity = 1 - n_ratings / rating_matrix_size\n",
    "\n",
    "    print(f\"Number of users: {n_users}\")\n",
    "    print(f\"Number of items: {n_items}\")\n",
    "    print(f\"Number of available ratings: {n_ratings}\")\n",
    "    print(f\"Number of all possible ratings: {rating_matrix_size}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"SPARSITY: {sparsity * 100.0:.2f}%\")\n",
    "\n",
    "\n",
    "print_sparsity(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * - Opcional: Plot para observar si hay long tail property: (CUIDADO CON NOMBRES DE COLS)\n",
    "popular_products = pd.DataFrame(df_sample.groupby(\"ProductId\")[\"Rating\"].count())\n",
    "most_popular = popular_products.sort_values(\"Rating\", ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "# First plot\n",
    "axes[0].bar(\n",
    "    x=range(len(item_rate_count)),\n",
    "    height=item_rate_count.values,\n",
    "    width=5.0,\n",
    "    align=\"edge\",\n",
    ")\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set(\n",
    "    title=\"Long tail of rating frequency\",\n",
    "    xlabel=\"Item ordered by decreasing frequency\",\n",
    "    ylabel=\"#Ratings\",\n",
    ")\n",
    "\n",
    "# Second plot adaptation\n",
    "# Assuming most_popular is a Series. If it's a DataFrame, you might need to adjust this part.\n",
    "x_pos = range(len(most_popular.head(30)))  # Generate x positions\n",
    "axes[1].bar(x=x_pos, height=most_popular.head(30)[\"Rating\"], align=\"center\")\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(most_popular.head(30).index, rotation=\"vertical\")\n",
    "axes[1].set(\n",
    "    title=\"Top 30 Most Popular Items\",\n",
    "    xlabel=\"Item\",\n",
    "    ylabel=\"Frequency or some other metric\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendation Systems EDA <a id='eda-section5'></a>\n",
    "* Generar la matriz de ratings \n",
    "* EDA de la matriz de ratings \n",
    "* User-Profiling \n",
    "* Item-Profiling\n",
    "* Distirbución de ratings por producto \n",
    "* Distribución de ratings por usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Generar la matriz de ratings:\n",
    "ratings_matrix = df_sample.pivot_table(\n",
    "    index=\"UserId\",\n",
    "    columns=\"ProductId\",\n",
    "    values=\"Rating\",\n",
    ")\n",
    "\n",
    "# * EDA de la matriz de ratings:\n",
    "ratings_matrix.head()\n",
    "df = ratings_matrix\n",
    "df[\"Mean Rating\"] = df.mean(axis=1)  # ? - Get the mean score for each user\n",
    "sns.histplot(\n",
    "    x=\"Mean Rating\", binwidth=0.5, data=df\n",
    ")  # ? - Histograma de la media de puntuación\n",
    "\n",
    "# * PARA PODER HACER ITEM Y USER PROFILING\n",
    "# * User-profiling para User-based\n",
    "# ? - Dataset para agrupar los items\n",
    "df_user_10k = pd.read_csv(\"path.csv\").set_index(\"UserId\").drop(\"Timestamp\", axis=1)\n",
    "items = df_user_10k.groupby(\n",
    "    \"ProductId\"\n",
    ")  # ? - Obtener lista de productos criticados por usuario\n",
    "items.get_group(\"B002OVV7F0\")  # ? - Pass ProductId - Get the ratings\n",
    "\n",
    "# ? - Dataset para agrupar los users\n",
    "df_item_10k = pd.read_csv(\"path.csv\").set_index(\"ProductId\").drop(\"Timestamp\", axis=1)\n",
    "users = df_item_10k.groupby(\"UserId\")  # ? - Obtener lista de usuarios por producto\n",
    "users.get_group(\"A39HTATAQ9V7YF\")  # ? - Pass UserId - Get the ratings for a user\n",
    "\n",
    "# ? - Observar distribución ratings producto especifico:\n",
    "df_item_10k.loc[\"B0050QLE4U\"].hist()\n",
    "# ? - Observar distribución ratings user especifico:\n",
    "df_user_10k.loc[\"A39HTATAQ9V7YF\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# PCA (Principal Component Analysis)  <a id=\"section2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparación del dataset + Normalizar <a id='pca-section1'></a>\n",
    "\n",
    "* Copiar el dataset \n",
    "* Definición de columnas categoricas y numericas\n",
    "* Train/Test/Split\n",
    "* Aplicar Standard Scaler para normalizar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#! En caso que nos den un archivo normal: \n",
    "df = pd.read_csv('path_to_csv.csv')\n",
    "dfY = df['target']\n",
    "dfX = df.drop('target', axis=1, inplace=True)\n",
    "\n",
    "# ? - En caso que los quieras volver a unir \n",
    "df = dfX.copy()\n",
    "df['target'] = dfY\n",
    "\n",
    "#! En caso que nos den algo con alguna libería rara como sklearn y load_breast_cancer:\n",
    "dataset = load_breast_cancer()\n",
    "X_data = dataset.data\n",
    "Y_data = dataset.target\n",
    "\n",
    "dfX = pd.DataFrame(X_data, columns=dataset.feature_names)\n",
    "dfY = pd.DataFrame(Y_data, columns=[\"target\"])\n",
    "\n",
    "df = dfX.copy()\n",
    "df['target'] = dfY\n",
    "\n",
    "# * - En caso que sea un dataset normal: Copiar el dataset con el fin de hacer operaciones + define columnas\n",
    "df_pca = df.copy()\n",
    "df_pca.head()\n",
    "\n",
    "# ? - Por si tienes que distinguir entre categoricas y numericas, creo que en el examen nos dará numericas\n",
    "CATEGORICAL_COLUMNS = [\n",
    "    \"TIME_OF_DAY\",\n",
    "    \"AIRCRAFT\",\n",
    "    \"AC_MASS\",\n",
    "    \"PHASE_OF_FLIGHT\",\n",
    "    \"SPECIES\",\n",
    "    \"NUM_STRUCK\",\n",
    "]\n",
    "\n",
    "NUMERICAL_COLUMNS = [\"INCIDENT_YEAR\", \"HEIGHT\", \"SPEED\", \"DISTANCE\", \"COST_INFL_ADJ\"]\n",
    "\n",
    "# * - Paso II: Solo aplicar el PCA a las variables numericas. Convertir variables si necesario\n",
    "# * - Despues arrancamos haciendo el train-test split:\n",
    "X = df_pca[NUMERICAL_COLUMNS] # ! - No te olvides de modificar todo esto\n",
    "X = X.drop(columns=[\"TARGET_VAR\"])  # * Variable target a identificar\n",
    "y = df_pca.TARGET_VAR  # * Continuous data for the darget\n",
    "\n",
    "# * - Train/Test Split\n",
    "X_train, X_test, y_train, y_tests = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# * - Paso III: Aplicamos el Standard Scaler con el objetivo de normalizar\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generar latent dims, aplicar PCA, determinar su error <a id='pca-section2'></a>\n",
    "\n",
    "* Observar la dimensión D original de la matriz\n",
    "* Generar vector de dimensiones latentes\n",
    "* Aplicar PCA y determinar su error\n",
    "* Hacer plot del train set vs. test-set reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction error on MNIST vs number of latent dimensions used by PCA\n",
    "# * This is dimension D\n",
    "X_rank = np.linalg.matrix_rank(X_train)\n",
    "max_components = min(X_train.shape[0], X_train.shape[1]) - 1\n",
    "Ks = list(\n",
    "    range(1, max_components + 1)\n",
    ")  # Ajustando dinámicamente el rango basado en el tamaño del conjunto de datos\n",
    "# * - Distinta manera de obtenerlo\n",
    "#L_linspace = np.linspace(1, X_rank, 10, dtype=int) \n",
    "\n",
    "\n",
    "# ? - Vectors which are intended to store the RMSE values for train and test datasets.\n",
    "RMSE_train = []\n",
    "RMSE_test = []\n",
    "\n",
    "# * For each L which we want to test out perform a PCA and it's corresponding reconstruction\n",
    "# * Both for the train & test datasets - Then for that specific L, log what the RMS error is in order to plot it.\n",
    "RMSE_train = []\n",
    "RMSE_test = []\n",
    "\n",
    "for index, K in enumerate(Ks):\n",
    "    pca = PCA(n_components=K)\n",
    "\n",
    "    Xtrain_transformed = pca.fit_transform(X_train)\n",
    "    Xtrain_proj = pca.inverse_transform(Xtrain_transformed)\n",
    "    RMSE_train.append(root_mean_squared_error(X_train, Xtrain_proj))\n",
    "\n",
    "    Xtest_transformed = pca.transform(X_test)\n",
    "    Xtest_proj = pca.inverse_transform(Xtest_transformed)\n",
    "    RMSE_test.append(root_mean_squared_error(X_test, Xtest_proj))\n",
    "\n",
    "# * Plot train set reconstruction error vs. test set reconstruction error:\n",
    "# Configurar la figura y los ejes para dos subplots: uno al lado del otro\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # 1 fila, 2 columnas\n",
    "\n",
    "# Gráfico para el conjunto de entrenamiento\n",
    "axs[0].plot(Ks, RMSE_train, marker=\"o\", color=\"blue\")\n",
    "axs[0].set_title(\"Train Set Reconstruction Error\")\n",
    "axs[0].set_xlabel(\"Number of Principal Components\")\n",
    "axs[0].set_ylabel(\"RMSE\")\n",
    "\n",
    "# Gráfico para el conjunto de prueba\n",
    "axs[1].plot(Ks, RMSE_test, marker=\"x\", color=\"red\")\n",
    "axs[1].set_title(\"Test Set Reconstruction Error\")\n",
    "axs[1].set_xlabel(\"Number of Principal Components\")\n",
    "# axs[1].set_ylabel(\"RMSE\")  # Opcional, ya que comparten el mismo eje Y\n",
    "\n",
    "plt.tight_layout()  # Ajustar automáticamente los parámetros de la subtrama para dar un relleno especificado\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicación del modelo PCA: <a id='pca-section3'></a>\n",
    "* Una vez hemos hecho el PCA para todos los valores y hemos observado el error de reconstrucción decidimos el modelo optimo\n",
    "  * Obtener los autovalores, varianza explicada + cumulative sum variance\n",
    "* Scree-plot\n",
    "* Log-Likelihood (Este no me esta saliendo muy bien)\n",
    "* Varianza cumulativa para establecer el threshold (elegir en base a esto la dim) + Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! - Como en el ultimo bucle hemos hecho PCA para todos los componentes, podemos sacar ahora el punto donde nuestro threshold se define: \n",
    "pca.components_.shape\n",
    "eigenvalues = pca.explained_variance_\n",
    "variance_explained = pca.explained_variance_ratio_\n",
    "cumulative_variance_explained = pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * - Screeplot\n",
    "fig, ax = plt.subplots()\n",
    "xs = np.arange(1, len(eigenvalues))\n",
    "ys = eigenvalues[0:len(eigenvalues)-1]\n",
    "plt.title(\"screeplot\")\n",
    "plt.xlabel(\"num PCs\")\n",
    "plt.ylabel(\"eigenvalues\")\n",
    "plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "ax.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Profile Likelihood\n",
    "def log_likelihood(evals):\n",
    "    Lmax = len(evals)\n",
    "    ll = np.arange(0.0, Lmax)\n",
    "\n",
    "    for L in range(Lmax):\n",
    "\n",
    "        group1 = evals[0 : L + 1]  # Divide Eigenvalues in two groups\n",
    "        group2 = evals[L + 1 : Lmax]\n",
    "\n",
    "        mu1 = np.mean(group1)\n",
    "        mu2 = np.mean(group2)\n",
    "\n",
    "        # eqn (20.30)\n",
    "        sigma = (np.sum((group1 - mu1) ** 2) + np.sum((group2 - mu2) ** 2)) / Lmax\n",
    "\n",
    "        ll_group1 = np.sum(multivariate_normal.logpdf(group1, mu1, sigma))\n",
    "        ll_group2 = np.sum(multivariate_normal.logpdf(group2, mu2, sigma))\n",
    "\n",
    "        ll[L] = ll_group1 + ll_group2\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = log_likelihood(eigenvalues)  # * Insert all of the corresponding eigenvalues.\n",
    "\n",
    "xs = np.arange(1, len(eigenvalues))\n",
    "ys = ll[0:len(eigenvalues)-1]\n",
    "\n",
    "plt.xlabel(\"num PCs\")\n",
    "plt.ylabel(\"profile log likelihood\")\n",
    "plt.plot(xs, ys)\n",
    "idx = np.argmax(ys)\n",
    "plt.axvline(xs[idx], c=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Cambia el threshold como veas relevante\n",
    "threshold = 0.9\n",
    "idx = np.where(cumulative_variance_explained > threshold)[0][0]\n",
    "exp_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "sns.lineplot(cumulative_variance_explained)\n",
    "plt.axvline(idx, c=\"r\")\n",
    "plt.axhline(exp_var[idx], c=\"r\")\n",
    "plt.xlabel(\"# PCA Components\")\n",
    "plt.ylabel(\"% Variance Explained\")\n",
    "plt.title(\"Number of Latent dimensions: \" + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "n_components = 2  #! - Definir\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X_train)\n",
    "X_transformed = pca.fit_transform(X_train)\n",
    "evals = pca.explained_variance_  # eigenvalues in descending order\n",
    "fraction_var = np.cumsum(evals[0:5] / np.sum(evals))   # ! - Change the dimension number to coincide with components\n",
    "\n",
    "# Access the matrix W of eigenvectors (principal components)\n",
    "W = pca.components_\n",
    "print(\"Matrix W (Eigenvectors/Principal Components):\")\n",
    "print(W)\n",
    "\n",
    "# Access the eigenvalues (explained variance)\n",
    "eigenvalues = pca.explained_variance_\n",
    "print(\"Eigenvalues (Explained Variance):\")\n",
    "print(eigenvalues)\n",
    "\n",
    "# Access the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\")\n",
    "print(explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Dimensión del PCA generado\n",
    "pca.components_.shape\n",
    "\n",
    "# * - Array de Autovalores\n",
    "eigenvalues = pca.explained_variance_\n",
    "eigenvalues\n",
    "\n",
    "# * - Array de Varianza\n",
    "variance_explained = pca.explained_variance_ratio_\n",
    "variance_explained\n",
    "\n",
    "# * - Cum Sum\n",
    "cum_variance_explained = pca.explained_variance_ratio_.cumsum()\n",
    "cum_variance_explained\n",
    "\n",
    "# * - Plot con el threshold para determinar latent dim optimo\n",
    "threshold = 0.9\n",
    "idx = np.where(cum_variance_explained > threshold)[0][0]\n",
    "exp_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "sns.lineplot(cum_variance_explained)\n",
    "plt.axvline(idx, c=\"r\")\n",
    "plt.axhline(exp_var[idx], c=\"r\")\n",
    "plt.xlabel(\"# PCA Components\")\n",
    "plt.ylabel(\"% Variance Explained\")\n",
    "plt.title(\"Number of Latent dimensions: \" + str(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averiguar el latent dim optimo haciendo plots <a id='pca-section4'></a>\n",
    "1. Generar la tabla para observar las propiedades de los PC.\n",
    "2. Visualización de los datos usando los dos latent dims más relevantes y observar similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal components correlation coefficients\n",
    "loadings = pca.components_\n",
    "\n",
    "# Number of features before PCA\n",
    "n_features = pca.n_features_in_\n",
    "\n",
    "# Feature names before PCA\n",
    "feature_names = NUMERICAL_COLUMNS #dfX.columns en caso que no hayas split \n",
    "\n",
    "# PC names\n",
    "pc_list = [f\"PC{i}\" for i in list(range(1, n_features + 1))]\n",
    "\n",
    "# Match PC names to loadings\n",
    "pc_loadings = dict(zip(pc_list, loadings))\n",
    "\n",
    "# Matrix of corr coefs between feature names and PCs\n",
    "loadings_df = pd.DataFrame.from_dict(pc_loadings)\n",
    "loadings_df[\"feature_names\"] = feature_names\n",
    "loadings_df = loadings_df.set_index(\"feature_names\")\n",
    "loadings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the loadings of x and y axes\n",
    "xs = loadings_df.iloc[:, 0]\n",
    "ys = loadings_df.iloc[:, 1]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "# Plot the loadings on a scatterplot\n",
    "for i, varnames in enumerate(feature_names):\n",
    "    plt.scatter(xs[i], ys[i], s=200)\n",
    "    plt.arrow(\n",
    "        0,\n",
    "        0,  # coordinates of arrow base\n",
    "        xs[i],  # length of the arrow along x\n",
    "        ys[i],  # length of the arrow along y\n",
    "        color=\"r\",\n",
    "        head_width=0.01,\n",
    "    )\n",
    "    plt.text(xs[i], ys[i], varnames)\n",
    "\n",
    "# Define the axes\n",
    "ymin = np.min(ys.iloc[i])\n",
    "ymax = np.max(ys.iloc[i])\n",
    "# Define the axes\n",
    "xticks = np.linspace(0, 0.5, num=10)\n",
    "yticks = np.linspace(ymin, ymax, num=10)\n",
    "plt.xticks(xticks)\n",
    "plt.yticks(yticks)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "\n",
    "# Show plot\n",
    "plt.title(\"2D Loading plot with vectors\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_components = sorted(\n",
    "    enumerate(variance_explained), key=lambda x: x[1], reverse=True\n",
    ")\n",
    "sorted_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Sección Clustering para hacerlo rapido de una. Cuando hayas acabado de hacer PCA <a id='important-exercise'></a>\n",
    "* Definir el modelo con train y test y el target \n",
    "* Generar grafica de accuracies \n",
    "* Hacer PCA \n",
    "* Matriz de confusión \n",
    "* Positive Class Classification \n",
    "* AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful function that help us with the dataframes\n",
    "def dataset_to_pandas(data, target):\n",
    "    data = pd.DataFrame(data, columns=[\"X\" + str(i + 1) for i in range(data.shape[1])])\n",
    "    inputs = data.columns\n",
    "    data[\"Y\"] = target\n",
    "    output = \"Y\"\n",
    "    return data, inputs, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Hacemos split para el dataset y estandarizamos\n",
    "data_std = StandardScaler().fit_transform(X_train)\n",
    "XTR, XTS, YTR, YTS = train_test_split(\n",
    "    data_std, y, test_size=0.2, random_state=1, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA in all datasets\n",
    "subspace_dim = 6\n",
    "pca = PCA(subspace_dim) # ! PCA con el numero optimo de lo que hallamos en la sección previa\n",
    "XTR_pca = pca.fit_transform(XTR) # * X Training\n",
    "XTS_pca = pca.fit_transform(XTS) # * X Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accrcies = []\n",
    "\n",
    "# Select the values of k neighbours\n",
    "k_start = 2\n",
    "k_stop = 50\n",
    "k_step = 1\n",
    "\n",
    "k_values = np.arange(start=k_start, stop=k_stop, step=k_step).astype(\"int\")\n",
    "\n",
    "train_data, train_inputs, train_outputs = dataset_to_pandas(XTR_pca, YTR)\n",
    "\n",
    "for k in k_values:\n",
    "    knn_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"knn\", KNeighborsClassifier(n_neighbors=k)),\n",
    "        ]\n",
    "    )\n",
    "    knn_pipe.fit(train_data[train_inputs], train_data[train_outputs])\n",
    "\n",
    "    accrcies.append(knn_pipe.score(train_data[train_inputs], train_data[train_outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accrcies = np.array(accrcies)\n",
    "# Plot accuracies vs k\n",
    "ax_acc = sns.scatterplot(x=k_values, y=accrcies)\n",
    "sns.lineplot(x=k_values, y=accrcies, ax=ax_acc)\n",
    "ax_acc.set(xlabel=\"k (num. of neighbors)\", ylabel=\"Accuracy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sección Cross-Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = np.arange(1, 50)\n",
    "hyp_grid = {\"knn__n_neighbors\": k_values}\n",
    "knn_pipe_hyper = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "\n",
    "knn_gridCV = GridSearchCV(\n",
    "    estimator=knn_pipe_hyper, param_grid=hyp_grid, cv=num_folds, return_train_score=True\n",
    ")\n",
    "\n",
    "knn_gridCV.fit(XTR_pca, YTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_gridCV.best_params_\n",
    "knn_gridCV.score(XTR_pca, YTR), knn_gridCV.score(XTS_pca, YTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matriz de confusión**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = knn_gridCV # * Matriz de Confusión\n",
    "fig = plt.figure(constrained_layout=True, figsize=(6, 2))\n",
    "spec = fig.add_gridspec(1, 2)\n",
    "ax1 = fig.add_subplot(spec[0, 0])\n",
    "ax1.set_title(\"Training\")\n",
    "ax1.grid(False)\n",
    "ax3 = fig.add_subplot(spec[0, 1])\n",
    "ax3.set_title(\"Test\")\n",
    "ax3.grid(False)\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    model, XTR_pca, YTR, cmap=\"Greens\", colorbar=False, ax=ax1, labels=[1, 0]\n",
    ")\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    model, XTS_pca, YTS, cmap=\"Greens\", colorbar=False, ax=ax3, labels=[1, 0]\n",
    ")\n",
    "plt.suptitle(\"Confusion Matrices\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fraction of positives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(constrained_layout=False, figsize=(12, 12))\n",
    "fig, ax = plt.subplots()\n",
    "CalibrationDisplay.from_estimator(\n",
    "    knn_gridCV, XTS_pca, YTS, n_bins=10, name=\"knn_pipe\", pos_label=1, ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curva AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 4))\n",
    "spec = fig.add_gridspec(1, 2)\n",
    "ax1 = fig.add_subplot(spec[0, 0])\n",
    "ax1.set_title(\"Training\")\n",
    "ax2 = fig.add_subplot(spec[0, 1])\n",
    "ax2.set_title(\"Test\")\n",
    "RocCurveDisplay.from_estimator(knn_gridCV, XTR_pca, YTR, plot_chance_level=True, ax=ax1)\n",
    "RocCurveDisplay.from_estimator(knn_gridCV, XTS_pca, YTS, plot_chance_level=True, ax=ax2)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Clustering <a id=\"section3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation: <a id='clustering-section1'></a>\n",
    "* Copiar el dataset\n",
    "* Coger variable predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering = df.copy()\n",
    "df_clustering.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculations <a id='clustering-section2'></a>\n",
    "* Purity Score\n",
    "* Rand Index\n",
    "* Adjusted Rand Index \n",
    "* Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):  # * Purity Score\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "\n",
    "purity_score(df_eval[\"Y\"], df_eval[\"Y_knn_pred\"])\n",
    "\n",
    "# * Rand Index\n",
    "rand_score(dfTR_eval[\"Y\"], dfTR_eval[\"Y_knn_pred\"])\n",
    "\n",
    "# * Adjusted Rand Score\n",
    "adjusted_rand_score(dfTR_eval[\"Y\"], dfTR_eval[\"Y_knn_pred\"])\n",
    "\n",
    "# * Normalized Mutual Info Score\n",
    "normalized_mutual_info_score(dfTR_eval[\"Y\"], dfTR_eval[\"Y_knn_pred\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Agglomerative Clustering (HAC) <a id='clustering-section3'></a>\n",
    "* Normal \n",
    "* Single-link\n",
    "* Complete-link\n",
    "* Average-link\n",
    "* Ward-Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(\n",
    "    X, method=\"single\"\n",
    ")  # ! - You can change the method from single to 'complete', 'ward', 'average', etc...\n",
    "\"\"\" #! - Descomentar este bloque si quieres hallar la fusión con mayor incremento de distancia. \n",
    "distances = linked[:, 2]\n",
    "# Calcular los incrementos de distancia entre fusiones sucesivas\n",
    "increments = np.diff(distances)\n",
    "\n",
    "# Identificar la fusión con el mayor incremento en distancia\n",
    "largest_increment_index = np.argmax(increments)\n",
    "# La distancia en la que ocurre esta fusión\n",
    "largest_increment_distance = distances[largest_increment_index]\n",
    "\"\"\"\n",
    "# labelList = range(1, 6)\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(\n",
    "    linked,\n",
    "    orientation=\"top\",\n",
    "    # labels=labelList,\n",
    "    distance_sort=\"descending\",\n",
    "    show_leaf_counts=True,\n",
    ")\n",
    "#! - plt.axhline(y=largest_increment_distance, c=\"k\", ls=\"--\", lw=0.5) <- Descomentar esta linea tambien si te conviene.\n",
    "plt.title(\"Dendrograma con la Mayor Fusión Indicada\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering - Asegurar de antemano que tienes valores numéricos y no categóricos <a id='clustering-section4'></a>\n",
    "* Elbow method para hallar el número optimo\n",
    "* Silhouette Score\n",
    "* Silhouette Score w/ clusters diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "\n",
    "# Rango de valores de k para probar\n",
    "k_values = range(2, 12)  # ! - Cambiar el rango\n",
    "\n",
    "kmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(df) for k in Ks]\n",
    "\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "\n",
    "silhouette_scores = [silhouette_score(df, model.labels_) for model in kmeans_per_k]\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(df)\n",
    "    inertias.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))  # * Graficar el método del codo\n",
    "plt.plot(k_values, inertias, \"-o\")\n",
    "plt.title(\"Método del Codo\")\n",
    "plt.xlabel(\"Número de Clusters, k\")\n",
    "plt.ylabel(\"Inercia\")\n",
    "plt.xticks(k_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()  # * Silhouette Score\n",
    "plt.plot(Ks, silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# * Silhouette Score visualization per cluster\n",
    "def plot_silhouette(model, X):\n",
    "    mu = model.cluster_centers_\n",
    "    K, D = mu.shape\n",
    "    y_pred = model.labels_\n",
    "    silhouette_coefficients = silhouette_samples(X, y_pred)\n",
    "    silhouette_scores = silhouette_score(X, model.labels_)\n",
    "    cmap = cm.get_cmap(\"Pastel2\")\n",
    "    colors = [cmap(i) for i in range(K)]\n",
    "    padding = len(X) // 30\n",
    "    pos = padding\n",
    "    for i in range(K):\n",
    "        coeffs = silhouette_coefficients[y_pred == i]\n",
    "        coeffs.sort()\n",
    "        color = mpl.cm.Spectral(i / K)\n",
    "        # color = colors[i]\n",
    "        plt.fill_betweenx(\n",
    "            np.arange(pos, pos + len(coeffs)),\n",
    "            0,\n",
    "            coeffs,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        pos += len(coeffs) + padding\n",
    "    score = silhouette_scores\n",
    "    plt.axvline(x=score, color=\"red\", linestyle=\"--\")\n",
    "    plt.title(\"$k={}, score={:0.2f}$\".format(K, score), fontsize=16)\n",
    "\n",
    "\n",
    "for model in kmeans_per_k:\n",
    "    K, D = model.cluster_centers_.shape\n",
    "    plt.figure()\n",
    "    plot_silhouette(model, df)\n",
    "    fname = f\"kmeans_silhouette_diagram{K}.pdf\"\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-Batches <a id='clustering-section5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 50\n",
    "times = np.empty((K, 2))\n",
    "inertias = np.empty((K, 2))\n",
    "\n",
    "for k in range(1, K + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    minibatch_kmeans = MiniBatchKMeans(\n",
    "        n_clusters=k, random_state=42\n",
    "    )  #! This is the minibatch declaration\n",
    "\n",
    "    start = time()\n",
    "    kmeans.fit(df)\n",
    "    times[k - 1, 0] = time() - start\n",
    "    inertias[k - 1, 0] = kmeans.inertia_\n",
    "\n",
    "    start = time()\n",
    "    minibatch_kmeans.fit(\n",
    "        df\n",
    "    )  # ! - Here is where we fit the mini-batch model for faster training\n",
    "    times[k - 1, 1] = time() - start\n",
    "    inertias[k - 1, 1] = minibatch_kmeans.inertia_\n",
    "\n",
    "# Graficar la inercia y los tiempos de entrenamiento\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Subplot para la inercia/distorsión\n",
    "plt.subplot(121)\n",
    "plt.plot(range(1, K + 1), inertias[:, 0], \"r--\", label=\"K-Means\")\n",
    "plt.plot(range(1, K + 1), inertias[:, 1], \"b.-\", label=\"Mini-batch K-Means\")\n",
    "plt.xlabel(\"$k$\", fontsize=16)\n",
    "plt.title(\"Distorsión\", fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "# Subplot para el tiempo de entrenamiento\n",
    "plt.subplot(122)\n",
    "plt.plot(range(1, K + 1), times[:, 0], \"r--\", label=\"K-Means\")\n",
    "plt.plot(range(1, K + 1), times[:, 1], \"b.-\", label=\"Mini-batch K-Means\")\n",
    "plt.xlabel(\"$k$\", fontsize=16)\n",
    "plt.title(\"Tiempo de entrenamiento (segundos)\", fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixture-Models <a id='clustering-section6'></a>\n",
    "* Definir Gaussian Mixture\n",
    "* Sacar sus pesos\n",
    "* Visualizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5  # ! - To modify\n",
    "gm = GaussianMixture(n_components=K, covariance_type=\"full\", n_init=10, random_state=42)\n",
    "gm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = gm.weights_\n",
    "mu = gm.means_\n",
    "Sigma = gm.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 100\n",
    "grid = np.arange(-10, 10, 1 / resolution)\n",
    "xx, yy = np.meshgrid(grid, grid)\n",
    "X_full = np.vstack([xx.ravel(), yy.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_samples is the log pdf\n",
    "pdf = np.exp(gm.score_samples(X_full))\n",
    "pdf_probas = pdf * (1 / resolution) ** 2\n",
    "print(\"integral of pdf {}\".format(pdf_probas.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing K. Co,mpare to kmeans_silhouette\n",
    "Ks = range(2, 9)\n",
    "gms_per_k = [\n",
    "    GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X) for k in Ks\n",
    "]\n",
    "\n",
    "bics = [model.bic(X) for model in gms_per_k]\n",
    "aics = [model.aic(X) for model in gms_per_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_full = GaussianMixture(\n",
    "    n_components=K, n_init=10, covariance_type=\"full\", random_state=42\n",
    ")\n",
    "gm_tied = GaussianMixture(\n",
    "    n_components=K, n_init=10, covariance_type=\"tied\", random_state=42\n",
    ")\n",
    "gm_spherical = GaussianMixture(\n",
    "    n_components=K, n_init=10, covariance_type=\"spherical\", random_state=42\n",
    ")\n",
    "gm_diag = GaussianMixture(\n",
    "    n_components=K, n_init=10, covariance_type=\"diag\", random_state=42\n",
    ")\n",
    "gm_full.fit(X)\n",
    "gm_tied.fit(X)\n",
    "gm_spherical.fit(X)\n",
    "gm_diag.fit(X)\n",
    "\n",
    "make_plot(\n",
    "    gm_full, X, \"full\"\n",
    ")  # * You can then switch the 'full' and the model for any of the ones defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering combined with PCA - MIRA MEJOR EL OTRO <a id='important-exercise-jic'></a>\n",
    "* **La probabilidad es que vayas a tener que hacer la sección de PCA del bloque anterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the dataset to preserve the original data\n",
    "df_pca = df.copy()\n",
    "\n",
    "# Define numerical columns\n",
    "NUMERICAL_COLUMNS = [\"INCIDENT_YEAR\", \"HEIGHT\", \"SPEED\", \"DISTANCE\", \"COST_INFL_ADJ\"]\n",
    "\n",
    "# * Select only numerical data\n",
    "X = df_pca[NUMERICAL_COLUMNS]\n",
    "y = df_pca[\"TARGET_VAR\"]  # Assuming you're interested in some target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# * - Estandarizamos los datos ya que a PCA le afectan las escalas y la varianza\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# ! - Para averiguar el numero optimo de clusters subir a la sección 2 del PCA\n",
    "pca = PCA()\n",
    "X_train_transformed = pca.fit_transform(X_train)\n",
    "\n",
    "# Calculate the cumulative variance explained by each component\n",
    "cum_variance_explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Determine the number of components that explain at least 90% of the variance\n",
    "threshold = 0.9\n",
    "num_components = np.where(cum_variance_explained >= threshold)[0][0] + 1\n",
    "\n",
    "print(\n",
    "    f\"Number of components to retain {num_components} which explain at least {threshold*100}% of variance\"\n",
    ")\n",
    "\n",
    "sns.lineplot(range(1, len(cum_variance_explained) + 1), cum_variance_explained)\n",
    "plt.axvline(num_components, color=\"r\", linestyle=\"--\")\n",
    "plt.xlabel(\"# PCA Components\")\n",
    "plt.ylabel(\"% Variance Explained\")\n",
    "plt.title(\"Explained Variance vs. Number of Components\")\n",
    "plt.show()\n",
    "\n",
    "# Perform PCA with the selected number of components\n",
    "pca_optimal = PCA(n_components=num_components)\n",
    "X_train_pca = pca_optimal.fit_transform(X_train)\n",
    "X_test_pca = pca_optimal.transform(X_test)\n",
    "\n",
    "# Test different numbers of clusters\n",
    "k_values = range(1, 11)\n",
    "inertias = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_train_pca)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow graph\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertias, \"-o\")\n",
    "plt.title(\"Elbow Method For Optimal k\")\n",
    "plt.xlabel(\"Number of Clusters, k\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.xticks(k_values)\n",
    "plt.show()\n",
    "\n",
    "optimal_k = 3  # Example based on the elbow plot observation\n",
    "final_kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "final_kmeans.fit(X_train_pca)\n",
    "\n",
    "# Assign clusters\n",
    "clusters = final_kmeans.labels_\n",
    "\n",
    "# Cluster centers\n",
    "cluster_centers = final_kmeans.cluster_centers_\n",
    "\n",
    "# Plotting cluster centers and data points in 2D (only possible if num_components >= 2)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    X_train_pca[:, 0],\n",
    "    X_train_pca[:, 1],\n",
    "    c=clusters,\n",
    "    cmap=\"viridis\",\n",
    "    marker=\"o\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "plt.scatter(\n",
    "    cluster_centers[:, 0], cluster_centers[:, 1], c=\"red\", marker=\"x\", s=100\n",
    ")  # Marking the cluster centers\n",
    "plt.title(\"Data points and cluster centers\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Rec. Systems - **Memory-Based** Collaborative Filtering (Cornac) <a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA for Rec. Systems - Just in case you forgot to do it before <a id='rs-section0'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Generar la matriz de ratings:\n",
    "ratings_matrix = df_sample.pivot_table(\n",
    "    index=\"UserId\",\n",
    "    columns=\"ProductId\",\n",
    "    values=\"Rating\",\n",
    ")\n",
    "\n",
    "# * EDA de la matriz de ratings:\n",
    "ratings_matrix.head()\n",
    "df_rm = ratings_matrix #! - Cuidado antes de pegar esta linea\n",
    "df_rm[\"Mean Rating\"] = df_rm.mean(axis=1)  # ? - Get the mean score for each user\n",
    "sns.histplot(\n",
    "    x=\"Mean Rating\", binwidth=0.5, data=df_rm\n",
    ")  # ? - Histograma de la media de puntuación\n",
    "\n",
    "# * PARA PODER HACER ITEM Y USER PROFILING\n",
    "# * User-profiling para User-based\n",
    "# ? - Dataset para agrupar los items\n",
    "df_user_10k = pd.read_csv(\"path.csv\").set_index(\"UserId\").drop(\"Timestamp\", axis=1)\n",
    "items = df_user_10k.groupby(\n",
    "    \"ProductId\"\n",
    ")  # ? - Obtener lista de productos criticados por usuario\n",
    "items.get_group(\"B002OVV7F0\")  # ? - Pass ProductId - Get the ratings\n",
    "\n",
    "# ? - Dataset para agrupar los users\n",
    "df_item_10k = pd.read_csv(\"path.csv\").set_index(\"ProductId\").drop(\"Timestamp\", axis=1)\n",
    "users = df_item_10k.groupby(\"UserId\")  # ? - Obtener lista de usuarios por producto\n",
    "users.get_group(\"A39HTATAQ9V7YF\")  # ? - Pass UserId - Get the ratings for a user\n",
    "\n",
    "# ? - Observar distribución ratings producto especifico:\n",
    "df_item_10k.loc[\"B0050QLE4U\"].hist()\n",
    "# ? - Observar distribución ratings user especifico:\n",
    "df_user_10k.loc[\"A39HTATAQ9V7YF\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-based filtering - Cornac <a id='rs-section1'></a>\n",
    "* Just in case you forgot to do the recommendation system EDA\n",
    "* Individual user and item exploration\n",
    "* User-Based Cornac Function (Pearson, Cosine, Mean-Centered)\n",
    "* User-Profiling\n",
    "* Score-prediction for K items for a specific user \n",
    "* Obtención del predicted ratings matrix\n",
    "* Predecir un score para usuario y item especifico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Normalise the ratings matrix by subtracting every user's rating by the mean users rating:\n",
    "normalized_ratings_matrix = ratings_matrix.subtract(ratings_matrix.mean(axis=1), axis=0)\n",
    "\n",
    "# * Generación de modelos user-based con Pearson, Cosine y centrados\n",
    "def userknn_cornac(df: pd.DataFrame):\n",
    "    # * Cuidado con el nombre de las columnas, a veces es userId, otras UserId\n",
    "    df = df.astype({\"UserId\": object, \"ProductId\": object})\n",
    "    records = df.to_records(index=False)\n",
    "    result = list(records)\n",
    "\n",
    "    K = 18  #! - Si te da error, baja el numero de k number of nearest neighbors\n",
    "    VERBOSE = False\n",
    "    SEED = 42\n",
    "    uknn_cosine = UserKNN(\n",
    "        k=K, similarity=\"cosine\", name=\"UserKNN-Cosine\", verbose=VERBOSE\n",
    "    )\n",
    "    uknn_cosine_mc = UserKNN(\n",
    "        k=K,\n",
    "        similarity=\"cosine\",\n",
    "        mean_centered=True,\n",
    "        name=\"UserKNN-Cosine-MC\",\n",
    "        verbose=VERBOSE,\n",
    "    )\n",
    "    uknn_pearson = UserKNN(\n",
    "        k=K, similarity=\"pearson\", name=\"UserKNN-Pearson\", verbose=VERBOSE\n",
    "    )\n",
    "    uknn_pearson_mc = UserKNN(\n",
    "        k=K,\n",
    "        similarity=\"pearson\",\n",
    "        mean_centered=True,\n",
    "        name=\"UserKNN-Pearson-MC\",\n",
    "        verbose=VERBOSE,\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    rec_300 = cornac.metrics.Recall(k=K)\n",
    "    prec_30 = cornac.metrics.Precision(k=K)\n",
    "    rmse = cornac.metrics.RMSE()\n",
    "    mae = cornac.metrics.MAE()\n",
    "\n",
    "    ratio_split = RatioSplit(result, test_size=0.1, seed=SEED, verbose=VERBOSE)\n",
    "    cornac.Experiment(\n",
    "        eval_method=ratio_split,\n",
    "        models=[uknn_cosine, uknn_cosine_mc, uknn_pearson, uknn_pearson_mc],\n",
    "        metrics=[rec_300, prec_30, rmse, mae],\n",
    "    ).run()\n",
    "\n",
    "    userknn_models = {\n",
    "        \"uknn_cosine\": uknn_cosine,\n",
    "        \"uknn_cosine_mc\": uknn_cosine_mc,\n",
    "        \"uknn_pearson\": uknn_pearson,\n",
    "        \"uknn_pearson_mc\": uknn_pearson_mc,\n",
    "    }\n",
    "\n",
    "    return userknn_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UserKNN methods #! - En caso que quieras aplicar el probability re-weighting, amplification - Si no quieres pasa a la siguiente celda\n",
    "K = 50  # number of nearest neighbors\n",
    "uknn_base = UserKNN(\n",
    "  k=K, similarity=\"pearson\", name=\"UserKNN-Base\", verbose=VERBOSE\n",
    ")\n",
    "uknn_amp1 = UserKNN(\n",
    "  k=K, similarity=\"pearson\", amplify=0.5, name=\"UserKNN-Amp0.5\", verbose=VERBOSE\n",
    ")\n",
    "uknn_amp2 = UserKNN(\n",
    "  k=K, similarity=\"pearson\", amplify=3.0, name=\"UserKNN-Amp3.0\", verbose=VERBOSE\n",
    ")\n",
    "uknn_idf = UserKNN(\n",
    "  k=K, similarity=\"pearson\", weighting=\"idf\", name=\"UserKNN-IDF\", verbose=VERBOSE\n",
    ")\n",
    "uknn_bm25 = UserKNN(\n",
    "  k=K, similarity=\"pearson\", weighting=\"bm25\", name=\"UserKNN-BM25\", verbose=VERBOSE\n",
    ")\n",
    "\n",
    "feedback = movielens.load_feedback(variant=\"100K\")\n",
    "ratio_split = RatioSplit(feedback, test_size=0.1, seed=SEED, verbose=VERBOSE)\n",
    "cornac.Experiment(\n",
    "  eval_method=ratio_split,\n",
    "  models=[uknn_base, uknn_amp1, uknn_amp2, uknn_idf, uknn_bm25],\n",
    "  metrics=[cornac.metrics.RMSE()],\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userknn_models = userknn_cornac(df_sample)  # ? - Returns the data with the Metrics\n",
    "model = userknn_models.get(\"uknn_cosine_mc\")\n",
    "# ?^^Luego tendras que justificar que modelo eliges. Esta bien que cojamos el mean centered (mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_profiling(UID, model, user_df, TOPK=5):\n",
    "\n",
    "    rating_mat = model.train_set.matrix\n",
    "\n",
    "    UIDX = list(model.train_set.uid_map.items())[UID][0]\n",
    "\n",
    "    print(f\"UserID = {UIDX}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(user_df.loc[UIDX])\n",
    "\n",
    "    ratings = pd.DataFrame(rating_mat.toarray())\n",
    "    user_ratings = ratings.loc[UID]\n",
    "    top_rated_items = np.argsort(user_ratings)[-TOPK:]\n",
    "    print(f\"\\nTOP {TOPK} RATED ITEMS BY USER {UID}:\")\n",
    "    print(\"-\" * 35)\n",
    "    print(user_df.iloc[top_rated_items.array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = pd.read_csv(\"dataset.csv\").set_index(\"userId\")\n",
    "df_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = userknn_models.get(\n",
    "    \"uknn_cosine_mc\"\n",
    ")  #!!! - SUPER IMPORTANTE HACER LA ELECCIÓN DEL MODELO EN BASE A LOS RESULTADOS\n",
    "top_rated_items = user_profiling(3, model, df_user, TOPK=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Predicción de score para cualquier producto:\n",
    "def uknn_get_scores(UID, model, user_df, TOPK=5):\n",
    "\n",
    "    UIDX = list(model.train_set.uid_map.items())[UID][0]\n",
    "    recommendations, scores = model.rank(UID)\n",
    "    print(f\"\\nTOP {TOPK} RECOMMENDATIONS FOR USER {UIDX}:\")\n",
    "    print(\"Scores:\", scores[recommendations[:TOPK]])\n",
    "    print(user_df.iloc[recommendations[:TOPK]])\n",
    "\n",
    "\n",
    "model = userknn_models.get('uknn_cosine') # * Decidimos de nuevo que modelo usar\n",
    "predicted_ratings = pd.DataFrame(\n",
    "    model.train_set.matrix.toarray(), \n",
    "    columns=list(model.train_set.iid_map.items()), \n",
    "    index=list(model.train_set.uid_map.items())\n",
    ")\n",
    "\n",
    "predicted_ratings.head()\n",
    "\n",
    "user_model = userknn_models.get(\"uknn_pearson_mc\") # * Decidimos el coeficiente de similitud \n",
    "item_mat_id = user_model.train_set.iid_map.get(\"B002I098JE\") # * Decidimos el item \n",
    "user_mat_id = user_model.train_set.uid_map.get(\"A7B5JEED0RKXG\") # * Decidimos el usuario\n",
    "# * Get the rating\n",
    "user_model.score(user_mat_id, item_mat_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item-Based filtering (Cornac) <a id='rs-section2'></a>\n",
    "\n",
    "* Function based on nearest k nearest neighbors with Pearson, Cosine, Centered\n",
    "* Item-profiling\n",
    "* Obtención del predicted ratings matrix\n",
    "* Como obtener una puntuación especifica de la ratings matrix predecida para usuario y item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps - Predict user's rating for one movie process:\n",
    "* 1. Create a list of the movies which the user 1 has watched and rated.\n",
    "* 2. Rank the similarities between the movies that user 1 has rated and the movie to predict.\n",
    "* 3. Select top n movies with the highest similarity scores.\n",
    "* 4. Calculate the predicted rating using weighted average of similarity scores and the ratings from user 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemknn_cornac(df):\n",
    "    # * Cuidado aquí con los nombres de las columnas\n",
    "    df = df.astype({\"UserId\": object, \"ProductId\": object})\n",
    "    records = df.to_records(index=False)\n",
    "    result = list(records)\n",
    "    K = 18  #!- number of nearest neighbors\n",
    "    VERBOSE = False\n",
    "    SEED = 42\n",
    "    iknn_cosine = ItemKNN(\n",
    "        k=K, similarity=\"cosine\", name=\"ItemKNN-Cosine\", verbose=VERBOSE\n",
    "    )\n",
    "    iknn_cosine_mc = ItemKNN(\n",
    "        k=K,\n",
    "        similarity=\"cosine\",\n",
    "        mean_centered=True,\n",
    "        name=\"ItemKNN-Cosine-MC\",\n",
    "        verbose=VERBOSE,\n",
    "    )\n",
    "    iknn_pearson = ItemKNN(\n",
    "        k=K, similarity=\"pearson\", name=\"ItemKNN-Pearson\", verbose=VERBOSE\n",
    "    )\n",
    "    iknn_pearson_mc = ItemKNN(\n",
    "        k=K,\n",
    "        similarity=\"pearson\",\n",
    "        mean_centered=True,\n",
    "        name=\"ItemKNN-Pearson-MC\",\n",
    "        verbose=VERBOSE,\n",
    "    )\n",
    "\n",
    "    # Metrics\n",
    "    rmse = cornac.metrics.RMSE()\n",
    "    mae = cornac.metrics.MAE()\n",
    "    prec = cornac.metrics.Precision(k=K)\n",
    "    ratio_split = RatioSplit(result, test_size=0.2, seed=SEED, verbose=VERBOSE)\n",
    "    cornac.Experiment(\n",
    "        eval_method=ratio_split,\n",
    "        models=[iknn_cosine, iknn_pearson, iknn_pearson_mc, iknn_cosine_mc],\n",
    "        metrics=[rmse, mae, prec],\n",
    "    ).run()\n",
    "    itemknn_models = {\n",
    "        \"iknn_cosine\": iknn_cosine,\n",
    "        \"iknn_pearson\": iknn_pearson,\n",
    "        \"iknn_pearson_mc\": iknn_pearson_mc,\n",
    "        \"iknn_cosine_mc\": iknn_cosine_mc,\n",
    "    }\n",
    "    return itemknn_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_profiling(UID, model, item_df, TOPK=5):\n",
    "\n",
    "    rating_mat = model.train_set.matrix\n",
    "\n",
    "    UIDX = list(model.train_set.iid_map.items())[UID][0]\n",
    "\n",
    "    print(f\"ProductID = {UIDX}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(item_df.loc[UIDX])\n",
    "\n",
    "    ratings = pd.DataFrame(rating_mat.toarray())\n",
    "    item_ratings = ratings.iloc[UID]\n",
    "    top_rated_items = np.argsort(item_ratings)[-TOPK:]\n",
    "    print(f\"\\nTOP {TOPK} RECOMMENDED USERS FOR ITEM {UID}:\")\n",
    "    print(\"-\" * 35)\n",
    "    print(df_item_10k.iloc[top_rated_items.array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemknn_get_scores(\n",
    "    UID, model, item_df, TOPK=5\n",
    "):  # ? En caso que quieras una función más especifica a los ratings: (Puedes pasar)\n",
    "    UIDX = list(model.train_set.iid_map.items())[UID][0]\n",
    "    recommendations, scores = model.rank(UID)\n",
    "    print(f\"\\nTOP {5} USERS FOR ITEM {UIDX}:\")\n",
    "    print(\"Scores:\", scores[recommendations[:TOPK]])\n",
    "    print(item_df.iloc[recommendations[:TOPK]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Create the models\n",
    "itemknn_models = itemknn_cornac(df)\n",
    "# * Pick the models\n",
    "model = itemknn_models.get(\n",
    "    \"iknn_cosine_mc\"\n",
    ")  #!!! - SUPER IMPORTANTE HACER LA ELECCIÓN DEL MODELO\n",
    "\n",
    "# * Get the top rated items for a specific user\n",
    "top_rated_items = item_profiling(2, model, df_item_10k)\n",
    "\n",
    "# * Indice del item, modelo seleccionado, y le pasas el dataset\n",
    "itemknn_get_scores(3, model, df_item_10k)\n",
    "\n",
    "# * Get the predictions matrix filled in\n",
    "predicted_ratings = pd.DataFrame(\n",
    "    model.train_set.matrix.toarray(), \n",
    "    columns=list(model.train_set.iid_map.items()), \n",
    "    index=list(model.train_set.uid_map.items())\n",
    ")\n",
    "\n",
    "predicted_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Obten el rating especifico para un producto y user:\n",
    "item_model = itemknn_models.get(\"iknn_cosine_mc\")\n",
    "item_mat_id = item_model.train_set.iid_map.get(\"B002I098JE\")\n",
    "user_mat_id = item_model.train_set.uid_map.get(\"A7B5JEED0RKXG\")\n",
    "item_model.score(user_mat_id, item_mat_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Rec. Systems - **Model-Based** Collaborative Filtering (Cornac) <a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA <a id='rs-section00'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Asegurate que tienes la matriz del pivotTable de antemano (ratings_matrix)\n",
    "# * En caso que no la tengas:\n",
    "ratings_matrix = df_sample.pivot_table(\n",
    "    index=\"UserId\",\n",
    "    columns=\"ProductId\",\n",
    "    values=\"Rating\",\n",
    ")\n",
    "\n",
    "# * Rellenamos la ratings matrix con la media\n",
    "mean_rating = 2.5\n",
    "r_df = ratings_matrix.fillna(mean_rating)\n",
    "\n",
    "# * Pasamos el df a numpy\n",
    "r = r_df.to_numpy()\n",
    "\n",
    "# * - Centramos los ratings al subtraer la media general de la matriz.\n",
    "user_ratings_mean = np.mean(r, axis=1)\n",
    "r_centered = r - user_ratings_mean.reshape(-1, 1)\n",
    "\n",
    "# * - Verificamos que la dimension se ha mantenido al subtraer\n",
    "# * - Multiplica los dos numeros del r.shape y ver si coincide con el count_nonzero\n",
    "print(r.shape)\n",
    "print(np.count_nonzero(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular Value Decomposition <a id='rs-section3'></a>\n",
    "* Generar el modelo SVD con K dimensión latente\n",
    "* Plot de K contra RMSE para elegir el K optimo\n",
    "* Modelo baseline (Solo con sesgos)\n",
    "* Recomendación de producto mediante SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_cornac( # * - Función para generar el módelo SVD\n",
    "    df, k_min=10, k_max=2000, step=100\n",
    "):  # ! - Modificar dependiendo del dataset\n",
    "    \"\"\"\n",
    "    Ejecuta experimentos SVD para un rango de valores de 'k' sobre un DataFrame utilizando Cornac.\n",
    "\n",
    "    Devuelve:\n",
    "    - (lista, cornac.Experiment): Lista de modelos SVD entrenados y el objeto experimento con los resultados.\n",
    "\n",
    "    \"\"\"\n",
    "    # * Cuidado aquí con las columnas - userId o UserId\n",
    "    df = df.astype({\"UserId\": object, \"ProductId\": object})\n",
    "    records = df.to_records(index=False)\n",
    "    result = list(records)\n",
    "\n",
    "    VERBOSE = False\n",
    "    SEED = 42\n",
    "\n",
    "    svd_models = []\n",
    "    k_values = np.arange(k_min, k_max, step)\n",
    "    for k in k_values:\n",
    "        svd_models.append(\n",
    "            SVD(\n",
    "                name=\"SVD\" + str(k),\n",
    "                k=k,\n",
    "                max_iter=30,\n",
    "                learning_rate=0.01,\n",
    "                lambda_reg=0.02,\n",
    "                verbose=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Metrics\n",
    "    rmse = cornac.metrics.RMSE()\n",
    "    mae = cornac.metrics.MAE()\n",
    "\n",
    "    ratio_split = RatioSplit(result, test_size=0.1, seed=SEED, verbose=VERBOSE)\n",
    "    svd_experiment = cornac.Experiment(\n",
    "        eval_method=ratio_split,\n",
    "        models=svd_models,\n",
    "        show_validation=True,\n",
    "        metrics=[rmse, mae],\n",
    "    )\n",
    "    svd_experiment.run()\n",
    "\n",
    "    return svd_models, svd_experiment\n",
    "\n",
    "svd_models, svd_experiment = svd_cornac(df_sample, 100, 500, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Función para hacer el plot comparando el número de dimensiones latentes vs. Error RMSE.\n",
    "# * Propósito: Obtener el K óptimo\n",
    "def plot_rmse_cornac(experiment, metric_name=\"RMSE\"):\n",
    "    metric_values = []\n",
    "    names_models = []\n",
    "    for i in range(len(experiment.result)):\n",
    "        metric_values.append(\n",
    "            svd_experiment.result[i].metric_avg_results.get(metric_name)\n",
    "        )\n",
    "        names_models.append(svd_experiment.result[i].model_name)\n",
    "\n",
    "    plt.xlabel(\"Latent Dimensions\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"SVD\")\n",
    "    plt.plot(names_models, metric_values, \"o-\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_rmse_cornac(svd_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! -  En caso de querer hacer un baseline model para SVD - Solo considerando los bias:\n",
    "df = df_sample.astype({\"UserId\": object, \"ProductId\": object})\n",
    "records = df.to_records(index=False)\n",
    "result = list(records)\n",
    "\n",
    "# ? - Instantiate an evaluation method to split data into train and test sets.\n",
    "ratio_split = cornac.eval_methods.RatioSplit(\n",
    "    data=df_sample.values, test_size=0.1, verbose=True\n",
    ")\n",
    "\n",
    "# ? - Instantiate the models of interest\n",
    "bo = cornac.models.BaselineOnly(\n",
    "    max_iter=30, learning_rate=0.01, lambda_reg=0.02, verbose=True\n",
    ")\n",
    "\n",
    "# Instantiate evaluation measures\n",
    "mae = cornac.metrics.MAE()\n",
    "rmse = cornac.metrics.RMSE()\n",
    "\n",
    "# Instantiate and run an experiment.\n",
    "cornac.Experiment(eval_method=ratio_split, models=[bo], metrics=[mae, rmse]).run()\n",
    "# ? - Tras ejecutar esto compara con el bloque de arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Función para recomendar a un usuario mediante el indice su producto\n",
    "def recommend_products(index, model, data, num_products=5):\n",
    "\n",
    "    print(\n",
    "        \"Name of Model:\", model.name\n",
    "    )  # ? - Sustituir el n, por el indice del modelo con menor RMSE\n",
    "\n",
    "    # Rank all test items for a given user.\n",
    "    df_rank = pd.DataFrame(\n",
    "        {\"ranked_items\": model.rank(index)[0], \"item_scores\": model.rank(index)[1]},\n",
    "        columns=[\"ranked_items\", \"item_scores\"],\n",
    "    )\n",
    "    print(\n",
    "        \"Target UserId\", data.iloc[index].UserId\n",
    "    )  # * Cuidado aqui con el df_smaple\n",
    "\n",
    "    df_rank.sort_values(\"item_scores\", ascending=False, inplace=True)\n",
    "\n",
    "    print(\n",
    "        \"Recommended products:\",\n",
    "        data.iloc[df_rank.head(num_products).ranked_items.values][\"ProductId\"].values,\n",
    "    )\n",
    "    print(\"Predicted scoreds: \", df_rank.head(num_products).item_scores.values)\n",
    "\n",
    "\n",
    "recommend_products(1, svd_models[n], df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorisation <a id='rs-section4'></a>\n",
    "* Option 1 - Single Matrix Factorisation model no comparison\n",
    "* Option 2 - Multiple models including Non-Negative Matrix Factorisation & Probabilistic Matrix Factorisation **<- Choose this one**\n",
    "  * Visualización de los puntos con los 2 factores con mayor varianza\n",
    "  * Aplicamos clustering en el espacio de los 2 factores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Factorises the ratings matrix into the product of two lower-rank matrices. \"Capturing the low-rank structure of the user-item interactions\".\n",
    "> * Y (mxn) => P (mxk) & $Q^T$ (kxn), where k << m, n is the latent factor size. So Y = $P*Q^T$\n",
    "> * P is the user matrix (m $\\rightarrow$ # of users) -> Rows measure user interest in item chars.\n",
    "> * Q is the item matrix (n $\\rightarrow$ # of items) -> Rows measure item characteristics set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Load the data for the MF\n",
    "# ? Option 1 - Non-csv\n",
    "data = movielens.load_feedback(variant=\"100K\")\n",
    "\n",
    "# !!!! - For when you want to produce a single MF model, SI NECESITAS COMPARAR VE ABAJO\n",
    "# ? !!! - Option 2 - CORNAC CSV IMPORT\n",
    "pandas_df = pd.read_csv(\"csv_path\")\n",
    "df = pandas_df.astype({\"UserId\": object, \"ProductId\": object})\n",
    "records = df.to_records(index=False)\n",
    "result = list(records)\n",
    "\n",
    "# * Data split and calculate the RMSE\n",
    "rs = RatioSplit(result, test_size=0.2, seed=SEED, verbose=VERBOSE)\n",
    "rmse = cornac.metrics.RMSE()\n",
    "\n",
    "K = 10\n",
    "lbd = 0.01  # ? Lambda -> Regularisation\n",
    "mf = MF(\n",
    "    k=K,\n",
    "    max_iter=20,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=lbd,\n",
    "    use_bias=False,\n",
    "    verbose=VERBOSE,\n",
    "    seed=SEED,\n",
    "    name=f\"MF(K={K},lambda={lbd:.4f})\",\n",
    ")\n",
    "\n",
    "# * Execute the MF model\n",
    "cornac.Experiment(eval_method=rs, models=[mf], metrics=[rmse]).run()\n",
    "# ? - ^^If you only have one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * NMF: Variant where the latent factors are constrained to be non-negative\n",
    "# * Ideal for non-negative factors like image processing, text mining, and rec. systems.\n",
    "# * As there are no negative factors.\n",
    "# * Allows for better interpretabiliy to reason with positive values:\n",
    "def mf_cornac(df, K=10):\n",
    "    df = df.astype({\"UserId\": object, \"ProductId\": object})\n",
    "    records = df.to_records(index=False)\n",
    "    result = list(records)\n",
    "    VERBOSE = False\n",
    "    SEED = 42\n",
    "    lbd = 0.01\n",
    "    baseline = BaselineOnly(\n",
    "        max_iter=20, learning_rate=0.01, lambda_reg=lbd, verbose=VERBOSE\n",
    "    )\n",
    "    mf1 = MF(\n",
    "        k=K,\n",
    "        max_iter=20,\n",
    "        learning_rate=0.01,\n",
    "        lambda_reg=0.0,\n",
    "        use_bias=False,\n",
    "        verbose=VERBOSE,\n",
    "        seed=SEED,\n",
    "        name=f\"MF(K={K})\",\n",
    "    )\n",
    "    mf2 = MF(\n",
    "        k=K,\n",
    "        max_iter=20,\n",
    "        learning_rate=0.01,\n",
    "        lambda_reg=lbd,\n",
    "        use_bias=False,\n",
    "        verbose=VERBOSE,\n",
    "        seed=SEED,\n",
    "        name=f\"MF(K={K},lambda={lbd:.4f})\",\n",
    "    )\n",
    "    mf3 = MF(\n",
    "        k=K,\n",
    "        max_iter=20,\n",
    "        learning_rate=0.01,\n",
    "        lambda_reg=lbd,\n",
    "        use_bias=True,\n",
    "        verbose=VERBOSE,\n",
    "        seed=SEED,\n",
    "        name=f\"MF(K={K},bias)\",\n",
    "    )\n",
    "    nmf = NMF(\n",
    "        k=K,\n",
    "        max_iter=200,\n",
    "        learning_rate=0.01,\n",
    "        use_bias=False,\n",
    "        verbose=VERBOSE,\n",
    "        seed=SEED,\n",
    "        name=f\"NMF(K={K})\",\n",
    "    )\n",
    "    ratio_split = RatioSplit(result, test_size=0.1, seed=SEED, verbose=VERBOSE)\n",
    "    cornac.Experiment(\n",
    "        eval_method=ratio_split,\n",
    "        models=[baseline, mf1, mf2, mf3, nmf],\n",
    "        metrics=[cornac.metrics.RMSE()],\n",
    "    ).run()\n",
    "\n",
    "    mf_models = {\"baseline\": baseline, \"mf1\": mf1, \"mf2\": mf2, \"mf3\": mf3, \"nmf\": nmf}\n",
    "    return mf_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "mf_models = mf_cornac(df_sample, K)\n",
    "\n",
    "model = mf_models.get(\"mf3\")  # ? - Selecciona el que mejor RMSE tenga\n",
    "var_df = pd.DataFrame(\n",
    "    {\"Factor\": np.arange(K), \"Variance\": np.var(model.i_factors, axis=0)}\n",
    ")\n",
    "\n",
    "# * Observar la info y varianza de cada dimensión latente del MF\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "plt.title(\"MF\")\n",
    "sns.barplot(x=\"Factor\", y=\"Variance\", data=var_df, hue=\"Factor\", legend=False, ax=ax)\n",
    "# * Con NMF estas forzando que los autovalores sean positivos, para poder mejorar su interpretabilidad\n",
    "# * PMF -> Probabilistic Matrix Factorisation (Tema 6)-> Y esa casi siempre mejora (Estudiaría así - Hazte un esquema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **K-Means para el Matrix Factorisation con los 2 factores con mayor varianza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP2F = (0, 2) # * Modificar a los factores más significativos\n",
    "SAMPLE_SIZE = 500\n",
    "\n",
    "mf = model\n",
    "rng = np.random.RandomState(42)\n",
    "sample_inds = rng.choice(np.arange(mf.i_factors.shape[0]), size=SAMPLE_SIZE)\n",
    "sample_df = pd.DataFrame(data=mf.i_factors[sample_inds][:, TOP2F], columns=[\"x\", \"y\"])\n",
    "sns.lmplot(x=\"x\", y=\"y\", data=sample_df, height=11.0, fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_centroids(data, k):\n",
    "    indexes = np.random.choice(len(data), size=k, replace=False)\n",
    "    centroids = data[indexes]\n",
    "    return centroids\n",
    "\n",
    "def assign_cluster(data, centroids):\n",
    "    # Pairwise squared L2 distances. Shape [n, k]\n",
    "    distances = ((data[:, np.newaxis] - centroids) ** 2).sum(axis=2)\n",
    "    # find closest centroid index. Shape [n]\n",
    "    clusters = np.argmin(distances, axis=1)\n",
    "    return clusters\n",
    "\n",
    "def update_centroids(data, clusters, k):\n",
    "    # Mean positions of data within clusters\n",
    "    centroids = [np.mean(data[clusters == i], axis=0) for i in range(k)]\n",
    "    return np.array(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMEANS:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, data, steps=20):\n",
    "        self.centroids = pick_centroids(data, self.k)\n",
    "        for step in range(steps):\n",
    "            clusters = assign_cluster(data, self.centroids)\n",
    "            self.centroids = update_centroids(data, clusters, self.k)\n",
    "\n",
    "    def predict(self, data):\n",
    "        return assign_cluster(data, self.centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMEANS(k=3)\n",
    "data = sample_df.to_numpy()\n",
    "kmeans.fit(data)\n",
    "clusters = kmeans.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1], c=clusters, cmap=\"viridis\", alpha=0.5)\n",
    "plt.scatter(\n",
    "    kmeans.centroids[:, 0],\n",
    "    kmeans.centroids[:, 1],\n",
    "    c=\"red\",\n",
    "    s=100,\n",
    "    edgecolor=\"black\",\n",
    "    label=\"Centroids\",\n",
    ")\n",
    "plt.title(\"Cluster Visualization with Centroids\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(clusterer, X, resolution=1000):\n",
    "    plt.figure()\n",
    "    mins = X.min(axis=0)\n",
    "    maxs = X.max(axis=0)\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(mins[0], maxs[0], resolution),\n",
    "        np.linspace(mins[1], maxs[1], resolution),\n",
    "    )\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.title(\"Cluster Visualization with Voronoi cells\")\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), cmap=\"Pastel2\")\n",
    "    plt.contour(\n",
    "        Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), linewidths=1, colors=\"k\"\n",
    "    )\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=clusters, cmap=\"viridis\", alpha=0.5)\n",
    "    plt.scatter(\n",
    "        kmeans.centroids[:, 0],\n",
    "        kmeans.centroids[:, 1],\n",
    "        c=\"red\",\n",
    "        s=100,\n",
    "        edgecolor=\"black\",\n",
    "        label=\"Centroids\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_decision_boundaries(kmeans, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Implicit Feedback - Interaction based (Cornac) <a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPR - Bayesian Probabilistic Ranking <a id='rs-section5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Import the data and split into train/test\n",
    "data = pd.read_csv(\"path_to_csv\")\n",
    "train, test = python_random_split(data, 0.75)\n",
    "train_set = cornac.data.Dataset.from_uir(train.itertuples(index=False), seed=SEED)\n",
    "\n",
    "# print(\"Number of users: {}\".format(train_set.num_users))\n",
    "# print(\"Number of items: {}\".format(train_set.num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * BPR Model\n",
    "# ? -top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# ? - Model parameters\n",
    "NUM_FACTORS = 250\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "bpr = cornac.models.BPR(\n",
    "    k=NUM_FACTORS,  # ? - Control the dimension of the latent space.\n",
    "    max_iter=NUM_EPOCHS,  # ? - Num of iterations for SGD\n",
    "    learning_rate=0.01,  # ? - Controls the step size alpha for gradient update. Small in this case\n",
    "    lambda_reg=0.001,  # ? - L2 Regularisation\n",
    "    verbose=True,\n",
    "    seed=SEED,\n",
    ").fit(\n",
    "    train_set\n",
    ")  # ? - In case you wish to train it directly\n",
    "\n",
    "# * The BPR model is effectively designed for item ranking. So we should only measure performance using the ranking metrics.\n",
    "with Timer() as t:\n",
    "    all_predictions = predict_ranking(\n",
    "        bpr, train, usercol=\"userID\", itemcol=\"itemID\", remove_seen=True\n",
    "    )\n",
    "print(f\"Took {t} secondes for the prediction\")\n",
    "\n",
    "all_predictions.head()  # ? - Visualise the prediction for user ratings\n",
    "bpr.rank(3)[1][\n",
    "    1394\n",
    "]  # ? - Get the ranking of items for user with ID 3 -> Access the second element with itemID 1394\n",
    "# TODO ^^ In the above case, shouldn't we apply a mask for the prediction for it to be zero?\n",
    "\n",
    "# * Analysis of the predictions and extract their performance matrix\n",
    "# Mean Average Precision for top k prediction items\n",
    "eval_map = map(test, all_predictions, col_prediction=\"prediction\", k=TOP_K)\n",
    "# Normalized Discounted Cumulative Gain (nDCG)\n",
    "eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction=\"prediction\", k=TOP_K)\n",
    "# precision at k (min=0, max=1)\n",
    "eval_precision = precision_at_k(\n",
    "    test, all_predictions, col_prediction=\"prediction\", k=TOP_K\n",
    ")\n",
    "eval_recall = recall_at_k(test, all_predictions, col_prediction=\"prediction\", k=TOP_K)\n",
    "\n",
    "print(\n",
    "    \"MAP:\\t%f\" % eval_map,\n",
    "    \"NDCG:\\t%f\" % eval_ndcg,\n",
    "    \"Precision@K:\\t%f\" % eval_precision,\n",
    "    \"Recall@K:\\t%f\" % eval_recall,\n",
    "    sep=\"\\n\",\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Matrix Factorisation: <a id='rs-section6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 50\n",
    "wmf = WMF(\n",
    "    k=K,\n",
    "    max_iter=100,\n",
    "    a=1.0,\n",
    "    b=0.01,\n",
    "    learning_rate=0.001,\n",
    "    lambda_u=0.01,\n",
    "    lambda_v=0.01,\n",
    "    verbose=VERBOSE,\n",
    "    seed=SEED,\n",
    "    name=f\"WMF(K={K})\",\n",
    ")\n",
    "\n",
    "eval_metrics = [\n",
    "    cornac.metrics.RMSE(),\n",
    "    cornac.metrics.AUC(),\n",
    "    cornac.metrics.Precision(k=10),\n",
    "    cornac.metrics.Recall(k=10),\n",
    "    cornac.metrics.FMeasure(k=10),\n",
    "    cornac.metrics.NDCG(k=[10, 20, 30]),\n",
    "    cornac.metrics.MRR(),\n",
    "    cornac.metrics.MAP(),\n",
    "]\n",
    "\n",
    "pandas_df = pd.read_csv(\"csv_path\")\n",
    "data = cornac.data.Dataset.from_uir(pandas_df.itertuples(index=False))\n",
    "rs = RatioSplit(data, test_size=0.2, seed=SEED, verbose=VERBOSE)\n",
    "cornac.Experiment(\n",
    "    eval_method=rs, models=[wmf, mf], metrics=eval_metrics\n",
    ").run()  # ? - This will output all of the metrics mentioned\n",
    "# * Consider that MF models are strong at predicting the ratings well.\n",
    "# * However, WMF models are designed to rank items, by fitting binary adoptions. (A click, a purchase, a view)\n",
    "# * This is more about showing interest, rather than judging how much they will like it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Factorisation Machines <a id='rs-section7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS device not found.\")\n",
    "\n",
    "# * - Get the item df\n",
    "item_df = pd.read_csv(\"path_to_csv\")\n",
    "# ? - Make sure to create a column with the Id index in case that the id's don't start as 0\n",
    "item_df[\"itemId_index\"] = item_df[\"itemId\"].astype(\"category\").cat.codes\n",
    "item_df.head()\n",
    "\n",
    "# * - Get the user df\n",
    "user_df = pd.read_csv(\"path_to_csv\")\n",
    "# ? - Remember to factorise all categorical variables !!! - Select those which are relevant\n",
    "user_df[\"gender_index\"] = user_df[\"gender\"].astype(\"category\").cat.codes\n",
    "user_df[\"age_index\"] = user_df[\"age\"].astype(\"category\").cat.codes\n",
    "user_df[\"occupation_index\"] = user_df[\"occupation\"].astype(\"category\").cat.codes\n",
    "user_df[\"userId_index\"] = user_df[\"userId\"].astype(\"category\").cat.codes\n",
    "user_df.head()\n",
    "\n",
    "# * - Get the ratings df and join it with the userId and itemId\n",
    "ratings_df = pd.read_csv(\"path_to_csv\")\n",
    "ratings = ratings_df.join(item_df.set_index(\"itemId\"), on=\"movieId\")\n",
    "ratings = ratings_df.join(user_df.set_index(\"userId\"), on=\"userId\")\n",
    "\n",
    "# * - Get the feature columns to prepare for Factor Machines. !!! Don't forget to modify for the real ones.\n",
    "# TODO - Is multi-fesature recommendation systems only relevant when it comes to implicit feedback?\n",
    "feature_columns = [\n",
    "    \"userId_index\",\n",
    "    \"itemId_index\",\n",
    "    \"age_index\",\n",
    "    \"gender_index\",\n",
    "    \"occupation_index\",\n",
    "]\n",
    "\n",
    "feature_sizes = {\n",
    "    \"userId_index\": len(ratings[\"userId_index\"].unique()),\n",
    "    \"movieId_index\": len(ratings[\"itemId_index\"].unique()),\n",
    "    \"age_index\": len(ratings[\"age_index\"].unique()),\n",
    "    \"gender_index\": len(ratings[\"gender_index\"].unique()),\n",
    "    \"occupation_index\": len(ratings[\"occupation_index\"].unique()),\n",
    "}\n",
    "\n",
    "# * Set the second order FM model made of three parts:\n",
    "# ? - 1. The offsets:\n",
    "next_offset = 0\n",
    "feature_offsets = {}\n",
    "\n",
    "# * This is in order to establish when to pass to the next feature\n",
    "for k, v in feature_sizes.items():\n",
    "    feature_offsets[k] = next_offset\n",
    "    next_offset += v\n",
    "\n",
    "# * Map all column indices to start from correct offset\n",
    "for column in feature_columns:\n",
    "    ratings[column] = ratings[column].apply(lambda c: c + feature_offsets[column])\n",
    "\n",
    "# * - Only visualise the feature columns along with the ratings, because that's what we need for FM.\n",
    "ratings[[*feature_columns, \"rating\"]].head(5)\n",
    "\n",
    "# * - Initialise the data and split it into train and test\n",
    "data_x = torch.tensor(ratings[feature_columns].values)\n",
    "data_y = torch.tensor(ratings[\"rating\"].values).float()\n",
    "dataset = data.TensorDataset(data_x, data_y)\n",
    "\n",
    "bs = 1024\n",
    "train_n = int(len(dataset) * 0.9)\n",
    "valid_n = len(dataset) - train_n\n",
    "splits = [train_n, valid_n]\n",
    "assert sum(splits) == len(dataset)  # ? - Verify that the split has been done correctly\n",
    "trainset, devset = torch.utils.data.random_split(\n",
    "    dataset, splits\n",
    ")  # ? - Assign the data to each split\n",
    "train_dataloader = data.DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "dev_dataloader = data.DataLoader(devset, batch_size=bs, shuffle=True)\n",
    "\n",
    "\n",
    "# * Function to fill in a tensor with a 'truncated distribution' -> mean 0, std 1\n",
    "# copied from fastai:\n",
    "def trunc_normal_(x, mean=0.0, std=1.0):\n",
    "    \"\"\"\n",
    "    Modifies a PyTorch tensor in-place, filling it with random values that approximate a truncated normal distribution.\n",
    "\n",
    "    This function fills the tensor `x` with values drawn from a standard normal distribution, then applies a modulus operation to limit the absolute values, and finally scales and shifts these values to achieve the desired mean and standard deviation. Note that the approach does not strictly adhere to a statistically accurate truncated normal distribution, as it does not cut off values outside a specific range but rather wraps them within a limited range.\n",
    "\n",
    "    Parameters:\n",
    "    - x (Tensor): The PyTorch tensor to be modified in-place.\n",
    "    - mean (float, optional): The mean of the distribution after adjustment. Defaults to 0.0.\n",
    "    - std (float, optional): The standard deviation of the distribution after adjustment. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: The modified tensor `x` with values approximating a truncated normal distribution centered around `mean` and with a standard deviation of `std`. The tensor is modified in-place, so the return value is the same tensor object `x`.\n",
    "    \"\"\"\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)\n",
    "\n",
    "\n",
    "class FMModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, n, k\n",
    "    ):  # ? - n: Number of unique features. k: Number of latent vectors\n",
    "        super().__init__()\n",
    "\n",
    "        self.w0 = nn.Parameter(torch.zeros(1))  # ? - Global bias\n",
    "        self.bias = nn.Embedding(n, 1)  # ? - Embedding layer for bias per feature\n",
    "        self.embeddings = nn.Embedding(\n",
    "            n, k\n",
    "        )  # ? - The actual embedding with dimension k\n",
    "\n",
    "        # ? - This initialises the embeddings and bias layers with a truncated normal distribution\n",
    "        with torch.no_grad():\n",
    "            trunc_normal_(self.embeddings.weight, std=0.01)\n",
    "        with torch.no_grad():\n",
    "            trunc_normal_(self.bias.weight, std=0.01)\n",
    "\n",
    "    def forward(\n",
    "        self, X\n",
    "    ):  # ? - How is the input tensor processed to produce a prediction?\n",
    "        emb = self.embeddings(X)  # ? - Compute embeddings for the input features\n",
    "        # ? - emb has shape: [batch_size, num_of_features, k]\n",
    "        # calculate the interactions in complexity of O(nk) see lemma 3.1 from paper\n",
    "        pow_of_sum = emb.sum(dim=1).pow(2)\n",
    "        sum_of_pow = emb.pow(2).sum(dim=1)\n",
    "        pairwise = (pow_of_sum - sum_of_pow).sum(1) * 0.5\n",
    "        bias = self.bias(X).squeeze().sum(1)\n",
    "        # I wrap the result with a sigmoid function to limit to be between 0 and 5.5.\n",
    "        return torch.sigmoid(self.w0 + bias + pairwise) * 5.5\n",
    "\n",
    "    # ? ^^Returns a sigmoid as the output will be limited between 0 and 1 -> The 5.5 I'm not sure why\n",
    "    # ? Probably because of the rating prediction.\n",
    "\n",
    "\n",
    "# fit/test functions\n",
    "def fit(iterator, model, optimizer, criterion):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for x, y in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x.to(device))\n",
    "        loss = criterion(y_hat, y.to(device))\n",
    "        train_loss += loss.item() * x.shape[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(iterator.dataset)\n",
    "\n",
    "\n",
    "def test(iterator, model, criterion):\n",
    "    train_loss = 0\n",
    "    model.eval()\n",
    "    for x, y in iterator:\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(x.to(device))\n",
    "        loss = criterion(y_hat, y.to(device))\n",
    "        train_loss += loss.item() * x.shape[0]\n",
    "    return train_loss / len(iterator.dataset)\n",
    "\n",
    "\n",
    "def train_n_epochs(model, n, optimizer, scheduler):\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    for epoch in range(n):\n",
    "        start_time = time.time()\n",
    "        train_loss = fit(train_dataloader, model, optimizer, criterion)\n",
    "        valid_loss = test(dev_dataloader, model, criterion)\n",
    "        scheduler.step()\n",
    "        secs = int(time.time() - start_time)\n",
    "        print(f\"epoch {epoch}. time: {secs}[s]\")\n",
    "        print(f\"\\ttrain rmse: {(math.sqrt(train_loss)):.4f}\")\n",
    "        print(f\"\\tvalidation rmse: {(math.sqrt(valid_loss)):.4f}\")\n",
    "\n",
    "\n",
    "model = FMModel(data_x.max() + 1, 20).to(device)\n",
    "wd = 1e-5\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[7], gamma=0.1)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = fit(train_dataloader, model, optimizer, criterion)\n",
    "    valid_loss = test(dev_dataloader, model, criterion)\n",
    "    scheduler.step()\n",
    "    secs = int(time.time() - start_time)\n",
    "    print(f\"epoch {epoch}. time: {secs}[s]\")\n",
    "    print(f\"\\ttrain rmse: {(math.sqrt(train_loss)):.4f}\")\n",
    "    print(f\"\\tvalidation rmse: {(math.sqrt(valid_loss)):.4f}\")\n",
    "\n",
    "# TODO: Aprender como ejecutar para coger la recomendación. Too abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Non-negative Matrix Factorisation (Puedes pasar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "nmf = NMF(\n",
    "    k=k,\n",
    "    max_iter=100,  # ? - How do we decide on the number of iterations\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.0,\n",
    "    verbose=VERBOSE,\n",
    "    seed=SEED,\n",
    "    name=f\"NMF (K = {k})\",\n",
    ")\n",
    "\n",
    "pandas_df = pd.read_csv(\"csv_path\")\n",
    "data = cornac.data.Dataset.from_uir(pandas_df.itertuples(index=False))\n",
    "\n",
    "rs = RatioSplit(data, test_size=0.2, seed=SEED, verbose=VERBOSE)\n",
    "rmse = cornac.metrics.RMSE()\n",
    "cornac.Experiment(eval_method=rs, models=[nmf], metrics=[rmse]).run()\n",
    "\n",
    "# ? - Visualise the variance for each latent factor in the NFM\n",
    "var_df = pd.DataFrame(\n",
    "    {\"Factor\": np.arange(K), \"Variance\": np.var(nmf.i_factors, axis=0)}\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "plt.title(\"NFM\")\n",
    "sns.barplot(x=\"Factor\", y=\"Variance\", data=var_df, palette=\"ch:.25\", ax=ax)\n",
    "\n",
    "# ? - Create a the reconstruction matrix based on the original dimensions\n",
    "recons_matrix = pd.DataFrame(\n",
    "    index=range(ratings_matrix.shape[0]), columns=range(ratings_matrix.shape[1])\n",
    ")\n",
    "# ? - Populate with the values\n",
    "for u, i in itertools.product(\n",
    "    range(recons_matrix.shape[0]), range(recons_matrix.shape[1])\n",
    "):\n",
    "    recons_matrix[u, i] = mf.score(u, i)\n",
    "# ? - ^^Careful if you had multiple models, this is for a single one.\n",
    "\n",
    "ratings_mask = (ratings_matrix > 0).astype(float)  # ? - To make them decimals\n",
    "\n",
    "rmse = np.sqrt((((ratings_matrix - recons_matrix) ** 2) * ratings_mask).mean())\n",
    "print(f\"\\nRMSE = {rmse:.3f}\")\n",
    "print(\"Reconstructed matrix:\")\n",
    "pd.DataFrame(\n",
    "    recons_matrix.round(2),\n",
    "    index=[f\"User {u + 1}\" for u in np.arange(df.num_users)],\n",
    "    columns=[f\"Item {i + 1}\" for i in np.arange(df.num_items)],\n",
    ")\n",
    "\n",
    "# * - Identify the top items associated with each latent factor in an NMF\n",
    "item_idx2id = list(nmf.train_set.item_ids)  # ? - Map the original id's of the items\n",
    "top_items = {}\n",
    "for k in range(K):  # ? - For each latent vector\n",
    "    # ? - For each column in the latent matrix, pick the top five items (Slice the last 5 items in ascending order. [::-1] then just reverses it)\n",
    "    top_inds = np.argsort(nmf.i_factors[:, k])[-5:][::-1]\n",
    "    # * Make sure you have an item df\n",
    "    # ? - Append to the dictionary the latent factor with its top 5 elements\n",
    "    top_items[f\"Factor {k}\"] = item_df.loc[[int(item_idx2id[i]) for i in top_inds]][\n",
    "        \"Title\"\n",
    "    ].values\n",
    "\n",
    "pd.DataFrame(top_items)\n",
    "\n",
    "# * Attempt to extract latent vector information by sorting into genre and see if they're related:\n",
    "item_idx2id = list(nmf.train_set.item_ids)\n",
    "top_genres = {}\n",
    "for k in range(K):\n",
    "    top_inds = np.argsort(nmf.i_factors[:, k])[-100:]  # ? - Same procedure\n",
    "    # ? - Make sure you have an item df\n",
    "    top_items = item_df.loc[\n",
    "        [int(item_idx2id[i]) for i in top_inds]\n",
    "    ]  # ? - Get the top films per latent ficture\n",
    "    # ? - Then drop the columns to just get the genre count.\n",
    "    top_genres[f\"Factor {k}\"] = top_items.drop(columns=[\"Title\", \"Release Date\"]).sum(\n",
    "        axis=0\n",
    "    )\n",
    "pd.DataFrame(top_genres)\n",
    "# TODO: Still don't have it clear how MF and SVD fill in the remaining elements !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Natural Language Processing <a id='section7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Introduction - Modelo Distilbert-base** <a id='nlp-section0'></a>\n",
    "* Clasificadores de frases para un modelo - una frase\n",
    "* Tokenizador de los modelos para frases singulares\n",
    "* Clasificación de multiples frases con torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis') # * Sin aplicar un modelo, coge un default\n",
    "res = classifier('I was sick last week')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#TODO - Do we need this or do we just pass the classifier name?\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "res = classifier([\"I hate you\", \"I love you\", \"I'm indifferent about you\"])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I hate you\" # * Tokeniser section for individual sentences\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tokenizer(sentence)\n",
    "\n",
    "# * Visualise Tokens, ID's, Model word ID's with the attention mask\n",
    "print(f\"Tokens:{tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Input IDs: {input_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\"I hate you\", \"I love you\"] # * Tokeniser section for multiple sentences\n",
    "batch = tokenizer(\n",
    "    X_train, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
    ")\n",
    "# ? - Padding -> All sequences must be padded to the same length\n",
    "# ? - Truncation -> Truncates any sequence longer than the specified length\n",
    "# ? - return_tensors -> pt for pytorch, tf for tensorflow \n",
    "\n",
    "with torch.no_grad():  # * Disables gradient calculations to save memory and compute, no backpropagation\n",
    "    # ? - Preprocessed data is fed into de model, ** is used ot unpack the dictionary into the models input arguments.\n",
    "    outputs = model(\n",
    "        **batch\n",
    "    )  \n",
    "\n",
    "    # ? - SequenceClassifier Output which includes logits [# of examples, # of classes] \n",
    "    # ? - We have two scores per instance corresponding to negative or positive.\n",
    "    # ? - Raw prediction of each class.\n",
    "    print(f\"Model output:\\n{outputs}\")\n",
    "\n",
    "    # ? - We apply a softmax to convert the logits to probabilities.\n",
    "    # ? -Dim = 1, specifies that we should apply them across the columns, normalising the logits into probabilities.\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    print(f\"\\nPrediction output:\\n{predictions}\")\n",
    "\n",
    "    # ? - Pick the index of the highest value in each row of predictions, which will give us the most likely class for a label\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(f\"\\nLabels:\\n{labels}\")\n",
    "\n",
    "    # ? - Finds the corresponding model label to the readable value\n",
    "    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **VADER** <a id='nlp-section1'></a>\n",
    "* Descarga de librerías | Importación y analisis de dataset | Tokens, Tags y entities\n",
    "* Sentiment Intensity Analysis sobre un dataset - *Bag of Words*\n",
    "* Matrix Factorisation with VADER (Modificación del dataset procesado) - **Necesario hacer la sección anterior**\n",
    "* Offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ? - Chunker es un modelo estadistico que permite identificar y clasificar entidades nombradas en el texto (Personas, Fechas, Empresas, Ubicaciones)\n",
    "# ? -  Se hace mediante chunking que agrupa PoS tags similares. \n",
    "nltk.download(\"maxent_ne_chunker\") \n",
    "# ? - Palabras en inglés del corpus WordNet - Util para correcciones, lematización, etc...\n",
    "nltk.download(\"words\") \n",
    "# ? - Esencial para el analisis de sentimientos con VADER. Es un diccionario que VADER usa para calificar la intensidad de los sentimientos de palabras y frases\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data_path'\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()\n",
    "\n",
    "# * Lo más común es que obtengamos un dataset con un userId, productId, Score y Texto\n",
    "# * Verifica esto en el head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ( # * Visualización de la clasificación de scores \n",
    "    df[\"Score\"] # ! - Esto podría ser facilmente el rating tambien\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .plot(kind=\"bar\", title=\"Count of Reviews by Stars\", figsize=(10, 5))\n",
    ")\n",
    "ax.set_xlabel(\"Review Stars\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Seleccionar un texto y aplicar tokenización +  Part of Speech Tagging\n",
    "example = df[\"Text\"][42] #! - Coges un ejemplo de una critica\n",
    "print(example)\n",
    "\n",
    "tokens = nltk.word_tokenize(example)   # * Tokenización del ejemplo.\n",
    "tagged = nltk.pos_tag(tokens)          # * Part of Speech Tagging - Significado Gramatico en la frase.\n",
    "entities = nltk.chunk.ne_chunk(tagged) # * Extracción de entidades de los speeches\n",
    "\n",
    "# ? - Visualizar los tokens y los PoS\n",
    "tokens[:10]\n",
    "tagged[:10]\n",
    "entities.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment Intensity Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Sentiment Analysis sobre una frase individual\n",
    "sentence = 'I am so happy!' #! - Esto lo puede re-emplazar con la critica\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(sentence) # * Output: neg | neu | pos | compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Sentiment analysis sobre todo el dataset: \n",
    "res = {}\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row[\"Text\"]\n",
    "    myid = row[\"Id\"]\n",
    "    res[myid] = sia.polarity_scores(text) # * Almacenamos en diccionario - clave Id del producto; valor el resultado del SIA\n",
    "\n",
    "# * Convertimos el resultado a un dataset y lo unimos a los datos\n",
    "vaders = pd.DataFrame(res).T\n",
    "vaders = vaders.reset_index().rename(columns={\"index\": \"Id\"})\n",
    "vaders = vaders.merge(df, how=\"left\")\n",
    "vaders.head() # * Ahora podemos relacionar el compound score con el rating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Analisis visual de la distribución de compound score\n",
    "ax = sns.barplot(data=vaders, x=\"Score\", y=\"compound\")\n",
    "ax.set_title(\"Compound Score by Amazon Star Review\")\n",
    "plt.show()\n",
    "\n",
    "# * Analisis visual de la distribución de neg/neu/pos\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 3))\n",
    "sns.barplot(data=vaders, x=\"Score\", y=\"pos\", ax=axs[0])\n",
    "sns.barplot(data=vaders, x=\"Score\", y=\"neu\", ax=axs[1])\n",
    "sns.barplot(data=vaders, x=\"Score\", y=\"neg\", ax=axs[2])\n",
    "axs[0].set_title(\"Positive\")\n",
    "axs[1].set_title(\"Neutral\")\n",
    "axs[2].set_title(\"Negative\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Factorisation Machine Data Prep w/ VADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaders.info() # * Para visualizar las categorías\n",
    "columns_to_drop = ['define', 'aqui', 'las', 'columnas', 'a', 'soltar']\n",
    "data = vaders.drop(columns_to_drop, axis=1)\n",
    "data.columns\n",
    "data.head() # * Deberías tener neg | neu | pos | compound | userId & productId | rating | alguna otra col. numérica\n",
    "data.info() # * Visualizar cols y tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * Aquí pasamos el Id a valores numéricos hash \n",
    "data[\"userId\"] = [hash(uid) for uid in data[\"UserId\"].values]\n",
    "data[\"productId\"] = [hash(uid) for uid in data[\"ProductId\"].values]\n",
    "data = data.drop([\"ProductId\", \"UserId\"], axis=1) # * Eliminamos ID columns para que todo sea numérico\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Esto prepara nuestro dataset para aplicarlo a un Factorisation Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    \"neg\",\n",
    "    \"neu\",\n",
    "    \"pos\",\n",
    "    \"compound\",\n",
    "    \"HelpfulnessNumerator\",\n",
    "    \"HelpfulnessDenominator\",\n",
    "    \"Time\",\n",
    "    \"userId\",\n",
    "    \"productId\",\n",
    "]\n",
    "\n",
    "features_sizes = {\n",
    "    \"userId\": data[\"userId\"].nunique(),\n",
    "    \"productId\": data[\"productId\"].nunique(),\n",
    "    \"HelpfulnessNumerator\": data[\"HelpfulnessNumerator\"].nunique(),\n",
    "    \"HelpfulnessDenominator\": data[\"HelpfulnessDenominator\"].nunique(),\n",
    "    \"Time\": data[\"Time\"].nunique(),\n",
    "    \"neg\": data[\"neg\"].nunique(),\n",
    "    \"neu\": data[\"neu\"].nunique(),\n",
    "    \"pos\": data[\"pos\"].nunique(),\n",
    "    \"compound\": data[\"compound\"].nunique(),\n",
    "}\n",
    "\n",
    "# * Calculate offsets.\n",
    "# * Each feature starts from the end of the last one.\n",
    "next_offset = 0\n",
    "features_offsets = {}\n",
    "for k, v in features_sizes.items():\n",
    "    features_offsets[k] = next_offset\n",
    "    next_offset += v\n",
    "\n",
    "features_offsets\n",
    "\n",
    "# * map all column indices to start from correct offset\n",
    "# * We take every value in a column, and we add the corresponding offset to that column.\n",
    "for column in feature_columns:\n",
    "    data[column] = data[column].apply(lambda c: c + features_offsets[column])\n",
    "\n",
    "data[[*feature_columns, \"Score\"]].head(\n",
    "    5\n",
    ")  # * This is a way to just display the columns we're interested in, and we unpack the feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ROBERTA** <a id='nlp-section2'></a>\n",
    "* Setup del modelo \n",
    "* Tokenización \n",
    "* Proceso para clasificar una frase\n",
    "* Proceso para hacerlo para todo un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * model - based on tweets\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I hate you\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tokenizer(sentence)\n",
    "\n",
    "print(f\"Tokens:{tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Input IDs: {input_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for Roberta Model\n",
    "encoded_text = tokenizer(sentence, return_tensors=\"pt\")\n",
    "print(f\"Example text: {sentence}\\nEncoded Text Output:{encoded_text}\\n\")\n",
    "\n",
    "# * We can observe from the output the negative, neutral and positive logits\n",
    "output = model(**encoded_text)\n",
    "print(f\"\\nOutput:\\n{output}\")\n",
    "\n",
    "# ? - Get the scores as logits and place it in a numpy vector\n",
    "scores = output[0][0].detach().numpy()\n",
    "# * Apply softmax to pass it to a probability and assign\n",
    "scores = softmax(scores)\n",
    "scores_dict = {\n",
    "    \"roberta_neg\": scores[0],\n",
    "    \"roberta_neu\": scores[1],\n",
    "    \"roberta_pos\": scores[2],\n",
    "}\n",
    "print(f\"\\nScores Dictionary final: {scores_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * File setup\n",
    "path = 'path_to_csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()\n",
    "\n",
    "reviews_per_rating = df[[\"ProductId\"]].value_counts()\n",
    "\n",
    "# !! - Group by Product, and select those which have more than 10 reviews like in the exam\n",
    "select_product = (reviews_per_rating >= 10).groupby(\"ProductId\").all()\n",
    "select_product = select_product.index[select_product].to_list()\n",
    "df = df.loc[df[\"ProductId\"].isin(select_product)]\n",
    "df.shape\n",
    "\n",
    "df.value_counts(\"Score\", normalize=True)\n",
    "\n",
    "# Numer of unique users and products\n",
    "n_users = df[\"UserId\"].nunique()\n",
    "print(\"UNIQUE USERS: \", n_users)\n",
    "n_products = df[\"ProductId\"].nunique()\n",
    "print(\"UNIQUE PRODUCTS: \", n_products)\n",
    "\n",
    "def print_sparsity(df):\n",
    "    n_users = df.UserId.nunique()\n",
    "    n_items = df.ProductId.nunique()\n",
    "    n_ratings = len(df)\n",
    "    rating_matrix_size = n_users * n_items\n",
    "    sparsity = 1 - n_ratings / rating_matrix_size\n",
    "\n",
    "    print(f\"Number of users: {n_users}\")\n",
    "    print(f\"Number of items: {n_items}\")\n",
    "    print(f\"Number of available ratings: {n_ratings}\")\n",
    "    print(f\"Number of all possible ratings: {rating_matrix_size}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"SPARSITY: {sparsity * 100.0:.2f}%\")\n",
    "\n",
    "\n",
    "print_sparsity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores_roberta(example):\n",
    "    encoded_text = tokenizer(example, return_tensors=\"pt\")\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {\n",
    "        \"roberta_neg\": scores[0],\n",
    "        \"roberta_neu\": scores[1],\n",
    "        \"roberta_pos\": scores[2],\n",
    "    }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    try:\n",
    "        text = row[\"Text\"]\n",
    "        myid = row[\"Id\"]\n",
    "        roberta_result = polarity_scores_roberta(str(text))\n",
    "        res[myid] = {**roberta_result}\n",
    "    except RuntimeError:\n",
    "        print(f\"Broke for id {myid}\")\n",
    "\n",
    "ratings = pd.DataFrame(res).T\n",
    "ratings = ratings.reset_index().rename(columns={\"index\": \"Id\"})\n",
    "ratings = ratings.merge(vaders, how=\"left\")\n",
    "ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so the wheel turns ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Graph networks <a id='section8'></a>\n",
    "\n",
    "Ejemplos en la pagina web: https://github.com/PreferredAI/cornac/tree/master/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Graph Modality**  <a id='na-section1'></a>\n",
    "* SoRec vs. PMF (Probabilistic Matrix Factorisation)\n",
    "* Interpreting Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20\n",
    "# * Definition of the SoRec Model\n",
    "sorec = SoRec(k=K, max_iter=50, learning_rate=0.001, verbose=VERBOSE, seed=SEED)\n",
    "# * Definition of the PMF model\n",
    "pmf = PMF(\n",
    "    k=K, max_iter=50, learning_rate=0.001, lambda_reg=0.01, verbose=VERBOSE, seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv('path_to_ratings.csv')\n",
    "trust_df = pd.read_csv('path_to_trust.csv')\n",
    "\n",
    "# ! Si nos piden que sea de la librería filmtrust aquí, si no, la siguiente sección: \n",
    "ratings = filmtrust.load_feedback()\n",
    "trust = filmtrust.load_trust() # * Trust define las relaciones entre usuarios. Como si son amigos o no\n",
    "user_graph_modality = GraphModality(data=trust)\n",
    "\n",
    "# ! En caso que no nos hagan extraer los datos del filmtrust pack de Cornac\n",
    "ratings = list(zip(ratings_df.user_id, ratings_df.item_id, ratings_df.rating))\n",
    "trust = list(zip(trust_df.user_id, trust_df.friend_id))\n",
    "user_graph_modality = GraphModality(data = trust)\n",
    "\n",
    "# * Split the dataset\n",
    "ratio_split = RatioSplit(\n",
    "    data=ratings,\n",
    "    test_size=0.2,\n",
    "    rating_threshold=2.5,\n",
    "    exclude_unknowns=True,\n",
    "    user_graph=user_graph_modality,\n",
    "    verbose=VERBOSE,\n",
    "    seed=SEED,\n",
    ")\n",
    "# * Error metrics\n",
    "mae = cornac.metrics.MAE()\n",
    "# * Execute the model to compare both the Social Recommendation vs. the normal Probabilistic Matrix Factorisation\n",
    "# * Si solo quieres el sorec, quita el pmf \n",
    "cornac.Experiment(eval_method=ratio_split, models=[sorec, pmf], metrics=[mae]).run() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretabilidad de los Factores y Visualización de las conexiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df = pd.DataFrame({\"Factor\": np.arange(K), \"Variance\": np.var(sorec.U, axis=0)})\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "# * Hacemos plot de los Factores vs. la varianza \n",
    "sns.barplot(\n",
    "    x=\"Factor\",\n",
    "    y=\"Variance\",\n",
    "    hue=\"Factor\",\n",
    "    data=var_df,\n",
    "    palette=\"ch:.25\",\n",
    "    ax=ax,\n",
    "    legend=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP2F = (9, 19) # ! - Definimos aquí los factores que más varianza contienen en la gráfica\n",
    "SAMPLE_SIZE = 200\n",
    "\n",
    "rng = np.random.RandomState(SEED)\n",
    "sample_inds = rng.choice(np.arange(sorec.U.shape[0]), size=SAMPLE_SIZE, replace=False)\n",
    "sample_df = pd.DataFrame(data=sorec.U[sample_inds][:, TOP2F], columns=[\"x\", \"y\"])\n",
    "g = sns.lmplot(x=\"x\", y=\"y\", data=sample_df, height=11.0, fit_reg=False)\n",
    "g.ax.set_title(\"Users in latent space with their social connections\", fontsize=16)\n",
    "\n",
    "adj_mat = sorec.train_set.user_graph.matrix\n",
    "for i in range(len(sample_inds)):\n",
    "    for j in range(len(sample_inds)):\n",
    "        if j != i and adj_mat[sample_inds[i], sample_inds[j]]:\n",
    "            sns.lineplot(x=\"x\", y=\"y\", data=sample_df.loc[[i, j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Text Modality** <a id='na-section2'></a>\n",
    "* Collaborative Topic Regression (CTR) vs. WMF (Weighted Matrix Factorisation)\n",
    "* Interpreting Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20\n",
    "ctr = CTR(\n",
    "    k=K,\n",
    "    max_iter=50,\n",
    "    a=1.0,\n",
    "    b=0.01,\n",
    "    lambda_u=0.01,\n",
    "    lambda_v=0.01,\n",
    "    verbose=VERBOSE,\n",
    "    seed=SEED,\n",
    ")\n",
    "wmf = WMF(\n",
    "    k=K,\n",
    "    max_iter=50,\n",
    "    a=1.0,\n",
    "    b=0.01,\n",
    "    learning_rate=0.005,\n",
    "    lambda_u=0.01,\n",
    "    lambda_v=0.01,\n",
    "    verbose=VERBOSE,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ! - En caso que no nos lo pidan de la librería Cornac:\n",
    "# Load the ratings data\n",
    "ratings_df = pd.read_csv('ratings.csv')  # Expected columns: ['user_id', 'item_id', 'rating']\n",
    "ratings = list(zip(ratings_df.user_id, ratings_df.item_id, ratings_df.rating))\n",
    "\n",
    "# Load the text data\n",
    "text_df = pd.read_csv('item_text.csv')  # Expected columns: ['item_id', 'text']\n",
    "docs = list(text_df.text)\n",
    "item_ids = list(text_df.item_id)\n",
    "\n",
    "# ! - En caso que si que lo pidan de la librería Cornac: \n",
    "ratings = amazon_clothing.load_feedback()\n",
    "docs, item_ids = amazon_clothing.load_text()\n",
    "\n",
    "# Prepare the Text Modality\n",
    "item_text_modality = TextModality(\n",
    "    corpus=docs,\n",
    "    ids=item_ids,\n",
    "    tokenizer=BaseTokenizer(sep=\" \", stop_words=\"english\"),\n",
    "    max_vocab=8000,\n",
    "    max_doc_freq=0.5\n",
    ")\n",
    "\n",
    "# Define the data split\n",
    "ratio_split = RatioSplit(\n",
    "    data=ratings,\n",
    "    test_size=0.2,\n",
    "    rating_threshold=4.0,\n",
    "    exclude_unknowns=True,\n",
    "    item_text=item_text_modality,  # Incorporating text modality here\n",
    "    verbose=VERBOSE,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "rec_50 = cornac.metrics.Recall(50)\n",
    "\n",
    "cornac.Experiment(eval_method=ratio_split, models=[ctr, wmf], metrics=[rec_50]).run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretability of the recommendation: \n",
    "* Get the top words of each topic\n",
    "* We can select the user and see what top topics their interested in & then see their recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ctr.train_set.item_text.vocab\n",
    "topic_word_dist = ctr.model.beta.T[:, -ctr.train_set.item_text.max_vocab :]\n",
    "top_word_inds = np.argsort(topic_word_dist, axis=1) + 4  # ingore 4 special tokens\n",
    "\n",
    "topic_words = {}\n",
    "topic_df = defaultdict(list)\n",
    "print(\"WORD TOPICS:\")\n",
    "for t in range(len(topic_word_dist)):\n",
    "    top_words = vocab.to_text(top_word_inds[t][-10:][::-1], sep=\", \")\n",
    "    topic_words[t + 1] = top_words\n",
    "    topic_df[\"Topic\"].append(t + 1)\n",
    "    topic_df[\"Top words\"].append(top_words)\n",
    "topic_df = pd.DataFrame(topic_df)\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UIDX = 123\n",
    "TOPK = 5\n",
    "\n",
    "item_id2idx = ctr.train_set.iid_map\n",
    "item_idx2id = list(ctr.train_set.item_ids)\n",
    "\n",
    "print(f\"USER {UIDX} TOP-3 TOPICS:\")\n",
    "topic_df.loc[np.argsort(ctr.U[UIDX])[-3:][::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations, scores = ctr.rank(UIDX)\n",
    "print(f\"\\nTOP {TOPK} RECOMMENDATIONS FOR USER {UIDX}:\")\n",
    "rec_df = defaultdict(list)\n",
    "for i in recommendations[:TOPK]:\n",
    "    rec_df[\"URL\"].append(f\"https://www.amazon.com/dp/{item_idx2id[i]}\")\n",
    "    rec_df[\"Description\"].append(ctr.train_set.item_text.corpus[i])\n",
    "pd.DataFrame(rec_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Image Modality y VBPR no entran por muy chulas que sean "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
